{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_path=\"data/dataset_train_2024.csv\"):\n",
    "        # Load data from CSV\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Extract features\n",
    "        self.sequences_1 = data.iloc[:, 1:129].values * 100  # Columns 1-128 (1-based indexing)\n",
    "        self.sequences_2 = data.iloc[:, 129:257].values * 100  # Columns 129-256\n",
    "        self.extra_feature = data.iloc[:, 257].values.reshape(-1, 1)  # Column 257\n",
    "\n",
    "        # Combine features\n",
    "        all_features = np.hstack([self.sequences_1, self.sequences_2, self.extra_feature])\n",
    "        \n",
    "        # Normalize features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.normalized_features = self.scaler.fit_transform(all_features)\n",
    "        self.features = torch.tensor(self.normalized_features, dtype=torch.float32)\n",
    "\n",
    "\n",
    "        # Encode labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = torch.tensor(self.label_encoder.fit_transform(data.iloc[:, -1]), dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "    \n",
    "    def inverseTransform(self, array):\n",
    "        return self.label_encoder.inverse_transform(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, bidirectional=False, dropout=0.0):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn = nn.GRU(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        # Adjust the output dimension if bidirectional is used\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # RNN forward pass\n",
    "        # Input shape: [batch_size, seq_len, input_dim]\n",
    "        out, _ = self.rnn(x)  # out: [batch_size, seq_len, hidden_dim]\n",
    "        # Use the last hidden state for classification\n",
    "        out = out[:, -1, :]  # [batch_size, hidden_dim]\n",
    "        out = self.fc(out)  # [batch_size, num_classes]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_tensor(tensor):\n",
    "    \"\"\"\n",
    "    Reformats a tensor from shape [batch_size, 257] to [batch_size, 128, 3] for RNN input.\n",
    "    \"\"\"\n",
    "    batch_size = tensor.shape[0]\n",
    "    # Extract sequences\n",
    "    seq1 = tensor[:, :128]  # First 128 features\n",
    "    seq2 = tensor[:, 128:256]  # Next 128 features\n",
    "    # Extract noise and expand it to match seq_len\n",
    "    noise = tensor[:, -1].unsqueeze(1).expand(batch_size, 128)\n",
    "    # Stack along the last dimension to form [batch_size, 128, 3]\n",
    "    return torch.stack([seq1, seq2, noise], dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "First try:\n",
    "\n",
    "hidden_dim = 64\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "bidirectional = True  # Set to False for unidirectional RNN\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "Second try (and best):\n",
    "\n",
    "hidden_dim = 32\n",
    "\n",
    "num_layers = 3\n",
    "\n",
    "bidirectional = True  # Set to False for unidirectional RNN\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training RNN model...\n",
      "Epoch 1/36, Loss: 1.3156\n",
      "Epoch 2/36, Loss: 0.3862\n",
      "Epoch 3/36, Loss: 0.2689\n",
      "Epoch 4/36, Loss: 0.1483\n",
      "Epoch 5/36, Loss: 0.1139\n",
      "Epoch 6/36, Loss: 0.0706\n",
      "Epoch 7/36, Loss: 0.0603\n",
      "Epoch 8/36, Loss: 0.0432\n",
      "Epoch 9/36, Loss: 0.0462\n",
      "Epoch 10/36, Loss: 0.0287\n",
      "Epoch 11/36, Loss: 0.0352\n",
      "Epoch 12/36, Loss: 0.0347\n",
      "Epoch 13/36, Loss: 0.0284\n",
      "Epoch 14/36, Loss: 0.0283\n",
      "Epoch 15/36, Loss: 0.0383\n",
      "Epoch 16/36, Loss: 0.0332\n",
      "Epoch 17/36, Loss: 0.0170\n",
      "Epoch 18/36, Loss: 0.0230\n",
      "Epoch 19/36, Loss: 0.0178\n",
      "Epoch 20/36, Loss: 0.0254\n",
      "Epoch 21/36, Loss: 0.0309\n",
      "Epoch 22/36, Loss: 0.0129\n",
      "Epoch 23/36, Loss: 0.0091\n",
      "Epoch 24/36, Loss: 0.0167\n",
      "Epoch 25/36, Loss: 0.0221\n",
      "Epoch 26/36, Loss: 0.0190\n",
      "Epoch 27/36, Loss: 0.0165\n",
      "Epoch 28/36, Loss: 0.0202\n",
      "Epoch 29/36, Loss: 0.0090\n",
      "Epoch 30/36, Loss: 0.0133\n",
      "Epoch 31/36, Loss: 0.0109\n",
      "Epoch 32/36, Loss: 0.0342\n",
      "Epoch 33/36, Loss: 0.0339\n",
      "Epoch 34/36, Loss: 0.0168\n",
      "Epoch 35/36, Loss: 0.0091\n",
      "Epoch 36/36, Loss: 0.0074\n",
      "Test Accuracy: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\numpy\\lib\\_function_base_impl.py:557: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "c:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "hidden_dim = 256\n",
    "num_layers = 3\n",
    "bidirectional = True  # Set to False for unidirectional RNN\n",
    "num_classes = 5\n",
    "learning_rate = 0.001\n",
    "epochs = 36\n",
    "batch_size = 32\n",
    "\n",
    "# Prepare Data\n",
    "dataset = CustomDataset()\n",
    "train_size = int(len(dataset))  # 80% for training\n",
    "test_size = 0 # len(dataset) - train_size  # 20% for testing\n",
    "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model Initialization\n",
    "model = RNNClassifier(\n",
    "    input_dim=3,  # This matches the last dimension of [batch_size, 128, 3]\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    bidirectional=bidirectional,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "print(\"Training RNN model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, labels in train_loader:\n",
    "        features = reformat_tensor(features).to(device)  # Reshape for RNN\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features = reformat_tensor(features).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(features)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n",
      "F1 Score: nan\n",
      "Accuracy: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\numpy\\lib\\_function_base_impl.py:557: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "c:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# Testing Loop\n",
    "print(\"Testing the model...\")\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        # Move data to the appropriate device\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        \n",
    "        # Reformat the input tensor to match the expected RNN input shape\n",
    "        features = reformat_tensor(features)\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = model(features)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        \n",
    "        # Collect predictions and labels for evaluation\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')  # or 'macro', 'micro', depending on your use case\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'predictions_RNN.csv'\n"
     ]
    }
   ],
   "source": [
    "#Using the model for prediction with the evaluation dataset\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define the dataset class\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.data = dataframe\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        normalized_values = self.scaler.fit_transform(self.data.values)\n",
    "        self.normalized_data = pd.DataFrame(\n",
    "            normalized_values, columns=self.data.columns, index=self.data.index\n",
    "        )\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.normalized_data.iloc[idx].values.astype('float32')  # Retrieve row as NumPy array\n",
    "        if self.transform:\n",
    "            inputs = self.transform(inputs)\n",
    "        return inputs\n",
    "\n",
    "# Load the unlabeled dataset\n",
    "csv_path = \"data/dataset_test_no_label_2024.csv\"  # Path to the dataset CSV file\n",
    "unlabeled_df = pd.read_csv(csv_path)  # Update the filename\n",
    "unlabeled_df = unlabeled_df.drop(unlabeled_df.columns[0], axis=1)\n",
    "unlabeled_dataset = UnlabeledDataset(unlabeled_df)\n",
    "unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Store predictions and indices\n",
    "predictions = []\n",
    "indices = []\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    for idx, inputs in enumerate(unlabeled_dataloader):\n",
    "        inputs = inputs.to(device)  # Send inputs to the same device as the model\n",
    "        \n",
    "        # Reformat inputs to match RNN input shape\n",
    "        inputs = reformat_tensor(inputs)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)  # Get predicted class\n",
    "        \n",
    "        # Save predictions and indices\n",
    "        start_idx = idx * unlabeled_dataloader.batch_size\n",
    "        batch_indices = list(range(start_idx, start_idx + len(inputs)))  # Adjusting the index properly\n",
    "        indices.extend(batch_indices)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Create a DataFrame with indices and predictions\n",
    "output_df = pd.DataFrame({\"ID\": indices, \"MODULATION\": dataset.inverseTransform(predictions)})\n",
    "\n",
    "# Save to a CSV file\n",
    "output_df.to_csv(\"predictions_RNN_4.csv\", index=False)\n",
    "\n",
    "print(\"Predictions saved to 'predictions_RNN.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./RNN/rnn_classifier_gridSearch_3_best.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model's state dictionary\n",
    "model_path = \"./RNN/rnn_classifier_gridSearch_3_best.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the saved model we use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricard\\AppData\\Local\\Temp\\ipykernel_22920\\4031045064.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(\"rnn_classifier.pth\"))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rnn_classifier.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 18\u001b[0m\n\u001b[0;32m      9\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m RNNClassifier(\n\u001b[0;32m     10\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39minput_dim,\n\u001b[0;32m     11\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     bidirectional\u001b[38;5;241m=\u001b[39mbidirectional\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Load the state dictionary\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m loaded_model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrnn_classifier.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     19\u001b[0m loaded_model\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Send the model to the appropriate device\u001b[39;00m\n\u001b[0;32m     20\u001b[0m loaded_model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rnn_classifier.pth'"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "input_dim = 3  # Match your input dimensions\n",
    "hidden_dim = 128  # Hidden dimension of your RNN\n",
    "num_layers = 2  # Number of RNN layers\n",
    "num_classes = 5  # Number of output classes\n",
    "bidirectional = False  # Match your model setting\n",
    "\n",
    "# Recreate the model\n",
    "loaded_model = RNNClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    bidirectional=bidirectional\n",
    ")\n",
    "\n",
    "# Load the state dictionary\n",
    "loaded_model.load_state_dict(torch.load(\"rnn_classifier.pth\"))\n",
    "loaded_model.to(device)  # Send the model to the appropriate device\n",
    "loaded_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, learning_rate, batch_size):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "            features = reformat_tensor(features)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "            \n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            features = reformat_tensor(features)\n",
    "            outputs = model(features)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    score = f1_score(all_labels, all_preds, average='weighted')\n",
    "    return accuracy, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training with hidden_dim=256, num_layers=2, lr=0.001, batch_size=32, dropout=0.2, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.9821\n",
      "Validation F1-Score: 0.9820\n",
      "Training with hidden_dim=256, num_layers=2, lr=0.001, batch_size=32, dropout=0.5, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.9833\n",
      "Validation F1-Score: 0.9833\n",
      "Training with hidden_dim=256, num_layers=3, lr=0.001, batch_size=32, dropout=0.2, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.9875\n",
      "Validation F1-Score: 0.9875\n",
      "Training with hidden_dim=256, num_layers=3, lr=0.001, batch_size=32, dropout=0.5, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.9875\n",
      "Validation F1-Score: 0.9875\n",
      "Training with hidden_dim=256, num_layers=4, lr=0.001, batch_size=32, dropout=0.2, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.9825\n",
      "Validation F1-Score: 0.9825\n",
      "Training with hidden_dim=256, num_layers=4, lr=0.001, batch_size=32, dropout=0.5, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.9900\n",
      "Validation F1-Score: 0.9900\n",
      "Training with hidden_dim=256, num_layers=5, lr=0.001, batch_size=32, dropout=0.2, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.9862\n",
      "Validation F1-Score: 0.9862\n",
      "Training with hidden_dim=256, num_layers=5, lr=0.001, batch_size=32, dropout=0.5, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.1958\n",
      "Validation F1-Score: 0.1073\n",
      "Training with hidden_dim=256, num_layers=6, lr=0.001, batch_size=32, dropout=0.2, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.1913\n",
      "Validation F1-Score: 0.0614\n",
      "Training with hidden_dim=256, num_layers=6, lr=0.001, batch_size=32, dropout=0.5, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.9825\n",
      "Validation F1-Score: 0.9825\n",
      "Training with hidden_dim=512, num_layers=2, lr=0.001, batch_size=32, dropout=0.2, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.9871\n",
      "Validation F1-Score: 0.9871\n",
      "Training with hidden_dim=512, num_layers=2, lr=0.001, batch_size=32, dropout=0.5, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.9788\n",
      "Validation F1-Score: 0.9788\n",
      "Training with hidden_dim=512, num_layers=3, lr=0.001, batch_size=32, dropout=0.2, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.2537\n",
      "Validation F1-Score: 0.1912\n",
      "Training with hidden_dim=512, num_layers=3, lr=0.001, batch_size=32, dropout=0.5, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.8321\n",
      "Validation F1-Score: 0.8264\n",
      "Training with hidden_dim=512, num_layers=4, lr=0.001, batch_size=32, dropout=0.2, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.6842\n",
      "Validation F1-Score: 0.6230\n",
      "Training with hidden_dim=512, num_layers=4, lr=0.001, batch_size=32, dropout=0.5, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.1950\n",
      "Validation F1-Score: 0.0636\n",
      "Training with hidden_dim=512, num_layers=5, lr=0.001, batch_size=32, dropout=0.2, bidirectional=True\n",
      "Epoch 1/35, Loss: 0.0271\n",
      "Epoch 2/35, Loss: 0.0271\n",
      "Epoch 3/35, Loss: 0.0271\n",
      "Epoch 4/35, Loss: 0.0271\n",
      "Epoch 5/35, Loss: 0.0271\n",
      "Epoch 6/35, Loss: 0.0271\n",
      "Epoch 7/35, Loss: 0.0271\n",
      "Epoch 8/35, Loss: 0.0271\n",
      "Epoch 9/35, Loss: 0.0271\n",
      "Epoch 10/35, Loss: 0.0271\n",
      "Epoch 11/35, Loss: 0.0271\n",
      "Epoch 12/35, Loss: 0.0271\n",
      "Epoch 13/35, Loss: 0.0271\n",
      "Epoch 14/35, Loss: 0.0271\n",
      "Epoch 15/35, Loss: 0.0271\n",
      "Epoch 16/35, Loss: 0.0271\n",
      "Epoch 17/35, Loss: 0.0271\n",
      "Epoch 18/35, Loss: 0.0271\n",
      "Epoch 19/35, Loss: 0.0271\n",
      "Epoch 20/35, Loss: 0.0271\n",
      "Epoch 21/35, Loss: 0.0271\n",
      "Epoch 22/35, Loss: 0.0271\n",
      "Epoch 23/35, Loss: 0.0271\n",
      "Epoch 24/35, Loss: 0.0271\n",
      "Epoch 25/35, Loss: 0.0271\n",
      "Epoch 26/35, Loss: 0.0271\n",
      "Epoch 27/35, Loss: 0.0271\n",
      "Epoch 28/35, Loss: 0.0271\n",
      "Epoch 29/35, Loss: 0.0271\n",
      "Epoch 30/35, Loss: 0.0271\n",
      "Epoch 31/35, Loss: 0.0271\n",
      "Epoch 32/35, Loss: 0.0271\n",
      "Epoch 33/35, Loss: 0.0271\n",
      "Epoch 34/35, Loss: 0.0271\n",
      "Epoch 35/35, Loss: 0.0271\n",
      "Validation Accuracy: 0.1933\n",
      "Validation F1-Score: 0.0626\n",
      "Training with hidden_dim=512, num_layers=5, lr=0.001, batch_size=32, dropout=0.5, bidirectional=True\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "hidden_dims = [256, 512]\n",
    "num_layers = [2, 3, 4, 5, 6]\n",
    "learning_rates = [0.001]\n",
    "batch_sizes = [32]\n",
    "dropouts = [0.2, 0.5]\n",
    "bidirectionals = [True]\n",
    "num_classes = 5\n",
    "epochs = 35\n",
    "    \n",
    "best_accuracy = 0\n",
    "best_f1_score = 0\n",
    "best_params = None\n",
    "\n",
    "# Prepare Data\n",
    "dataset = CustomDataset()\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size  # 20% for testing\n",
    "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "hyperparameter_combinations = product(hidden_dims, num_layers, learning_rates, batch_sizes, dropouts, bidirectionals)\n",
    "\n",
    "for hidden_dim, num_layer, learning_rate, batch_size, dropout, bidirectional in hyperparameter_combinations:\n",
    "        print(f\"Training with hidden_dim={hidden_dim}, num_layers={num_layer}, lr={learning_rate}, batch_size={batch_size}, dropout={dropout}, bidirectional={bidirectional}\")\n",
    "        \n",
    "        # Create DataLoader for current batch_size\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize and train the model\n",
    "        model = RNNClassifier(input_dim=3, hidden_dim=hidden_dim, num_layers=num_layer, num_classes=num_classes, \n",
    "                              bidirectional=bidirectional, dropout=dropout).to(device)\n",
    "        \n",
    "        accuracy, score = train_model(model, train_loader, val_loader, epochs, learning_rate, batch_size)\n",
    "        \n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Validation F1-Score: {score:.4f}\")\n",
    "        \n",
    "        # Update best accuracy and parameters\n",
    "        if score > best_f1_score:\n",
    "            best_accuracy = accuracy\n",
    "            best_f1_score = score\n",
    "            best_params = {\n",
    "                  'hidden_dim': hidden_dim,\n",
    "                  'num_layers': num_layer,\n",
    "                  'learning_rate': learning_rate,\n",
    "                  'batch_size': batch_size,\n",
    "                  'dropout': dropout,\n",
    "                  'bidirectional': bidirectional\n",
    "                  }\n",
    "\n",
    "print(\"Best Parameters found:\", best_params)\n",
    "print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"Best Validation F1-Score: {best_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters found:\", best_params)\n",
    "print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"Best Validation F1-Score: {best_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN_GridSearch\n",
    "\n",
    "Best Parameters found: {'hidden_dim': 32, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 16, \n",
    "'dropout': 0.0, 'bidirectional': True}\n",
    "\n",
    "Best Validation Accuracy: 0.9900\n",
    "\n",
    "Best Validation F1-Score: 0.9900\n",
    "\n",
    "RNN_2 - Best one on kaggle\n",
    "\n",
    "Training with hidden_dim=64, num_layers=4, lr=0.001, batch_size=32, dropout=0.0, bidirectional=True\n",
    "\n",
    "Validation Accuracy: 0.9925\n",
    "\n",
    "Validation Accuracy: 0.9925\n",
    "\n",
    "Training with hidden_dim=256, num_layers=3, lr=0.001, batch_size=32, dropout=0.0, bidirectional=True\n",
    "\n",
    "Validation Accuracy: 0.9929\n",
    "\n",
    "Validation Accuracy: 0.9929\n",
    "\n",
    "GridSearch 3:\n",
    "\n",
    "Best Parameters found: {'hidden_dim': 256, 'num_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'bidirectional': True, dropout: 0.2}\n",
    "\n",
    "Best Validation Accuracy: 0.9912\n",
    "\n",
    "Best Validation F1-Score: 0.9912"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
