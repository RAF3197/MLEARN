{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the multiHead attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    \"\"\" Computes the Scaled Dot-Product Attention\n",
    "\n",
    "    Args:\n",
    "        q (torch.FloatTensor):  Query Tensor   (... x T_q x d_q)\n",
    "        k (torch.FloatTensor):  Key Tensor     (... x T_k x d_k)\n",
    "        v (torch.FloatTensor):  Value Tensor   (... x T_v x d_v)\n",
    "        mask (torch.BoolTensor): Attention mask (... x T_q x T_k)\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: Result of the SDPA  (... x T_q x d_v)\n",
    "        torch.FloatTensor: Attention map       (... x T_q x T_k)\n",
    "\n",
    "    \"\"\"\n",
    "    assert q.size(-1) == k.size(-1), \"Query and Key dimensions must coincide\"\n",
    "\n",
    "    # TODO: Matrix multiplication of the queries and the keys (use torch.matmul)\n",
    "    #attn_logits =\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "\n",
    "    # TODO: Scale attn_logits (see the SDPA formula, d_k is the last dim of k)\n",
    "    #attn_logits = \n",
    "    attn_logits = attn_logits/torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))\n",
    "\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask, -float(\"inf\"))\n",
    "\n",
    "    # TODO: Compute the attention weights (see the SDPA formula, use dim=-1)\n",
    "    #attention =\n",
    "    attention = torch.softmax(attn_logits, dim=-1)\n",
    "\n",
    "    output = torch.matmul(attention, v)\n",
    "\n",
    "    return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \\\n",
    "            \"Embedding dimension must be multiple of the number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.proj_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization\n",
    "        nn.init.xavier_uniform_(self.proj_q.weight)\n",
    "        nn.init.xavier_uniform_(self.proj_k.weight)\n",
    "        nn.init.xavier_uniform_(self.proj_v.weight)\n",
    "        nn.init.xavier_uniform_(self.proj_o.weight)\n",
    "        self.proj_q.bias.data.fill_(0)\n",
    "        self.proj_k.bias.data.fill_(0)\n",
    "        self.proj_v.bias.data.fill_(0)\n",
    "        self.proj_o.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(1)\n",
    "\n",
    "        q = self.proj_q(q)\n",
    "        k = self.proj_k(k)\n",
    "        v = self.proj_v(v)\n",
    "\n",
    "        # TODO: Split the tensors into multiple heads\n",
    "        #  T x B x embed_dim -> T x B x num_heads x head_dim\n",
    "        q = q.reshape(q.size(0), batch_size, self.num_heads, self.head_dim)\n",
    "        k = k.reshape(k.size(0), batch_size, self.num_heads, self.head_dim)\n",
    "        v = v.reshape(v.size(0), batch_size, self.num_heads, self.head_dim)\n",
    "\n",
    "        # The last two dimensions must be sequence length and the head dimension,\n",
    "        # to make it work with the scaled dot-product function.\n",
    "        # TODO: Rearrange the dimensions\n",
    "        # T x B x num_heads x head_dim -> B x num_heads x T x head_dim\n",
    "        q = q.permute(1, 2, 0, 3)\n",
    "        k = k.permute(1, 2, 0, 3)\n",
    "        v = v.permute(1, 2, 0, 3)\n",
    "\n",
    "        # Apply the same mask to all the heads\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # TODO: Call the scaled dot-product function (remember to pass the mask!)\n",
    "        output_heads, attn_w = scaled_dot_product(q, k, v, mask)\n",
    "\n",
    "        # B x num_heads x T x head_dim -> T x B x num_heads x head_dim\n",
    "        output_heads = output_heads.permute(2, 0, 1, 3)\n",
    "\n",
    "        # T x B x num_heads x head_dim -> T x B x embed_dim\n",
    "        output_cat = output_heads.reshape(-1, batch_size, self.embed_dim)\n",
    "        output = self.proj_o(output_cat)\n",
    "\n",
    "        return output, attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimensionality\n",
    "            max_len (int): Maximum length of a sequence to expect\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create matrix of (T x embed_dim) representing the positional encoding\n",
    "        # for max_len inputs\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimensionality (input, output & self-attention)\n",
    "            ffn_dim (int): Inner dimensionality in the FFN\n",
    "            num_heads (int): Number of heads of the multi-head attention block\n",
    "            dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiheadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(ffn_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None, return_att=False):\n",
    "        src_len, batch_size, _ = x.shape\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(x.shape[1], x.shape[0]).bool().to(x.device)\n",
    "\n",
    "        selfattn_mask = mask.unsqueeze(-2)\n",
    "\n",
    "        # TODO: Self-Attention block\n",
    "        selfattn_out, selfattn_w = self.self_attn(x, x, x, selfattn_mask)\n",
    "        selfattn_out = self.dropout(selfattn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (1)\n",
    "        x = self.norm1(x + selfattn_out)\n",
    "\n",
    "        # TODO: FFN block\n",
    "        ffn_out = self.ffn(x)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (2)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "\n",
    "        if return_att:\n",
    "            return x, selfattn_w\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        # Create an embedding table (T x B -> T x B x embed_dim)\n",
    "        # self.embedding = nn.Embedding(, embed_dim)\n",
    "\n",
    "        # Create the positional encoding with the class defined before\n",
    "        self.pos_enc = PositionalEncoding(embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, ffn_dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None, return_att=False):\n",
    "        #x = self.embedding(x)\n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        selfattn_ws = []\n",
    "        for l in self.layers:\n",
    "            if return_att:\n",
    "                x, selfattn_w = l(x, mask=mask, return_att=True)\n",
    "                selfattn_ws.append(selfattn_w)\n",
    "            else:\n",
    "                x = l(x, mask=mask, return_att=False)\n",
    "\n",
    "        if return_att:\n",
    "            selfattn_ws = torch.stack(selfattn_ws, dim=1)\n",
    "            return x, selfattn_ws\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimensionality (input, output & self-attention)\n",
    "            ffn_dim (int): Inner dimensionality in the FFN\n",
    "            num_heads (int): Number of heads of the multi-head attention block\n",
    "            dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiheadAttention(embed_dim, num_heads)\n",
    "        self.encdec_attn = MultiheadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(ffn_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, memory, mask=None, memory_mask=None, return_att=False):\n",
    "        tgt_len, batch_size, _ = x.shape\n",
    "        src_len, _, _ = memory.shape\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(x.shape[1], x.shape[0])\n",
    "            mask = mask.bool().to(x.device)\n",
    "        if memory_mask is None:\n",
    "            memory_mask = torch.zeros(memory.shape[1], memory.shape[0])\n",
    "            memory_mask = memory_mask.bool().to(memory.device)\n",
    "\n",
    "\n",
    "        subsequent_mask = torch.triu(torch.ones(batch_size, tgt_len, tgt_len), 1)\n",
    "        subsequent_mask = subsequent_mask.bool().to(mask.device)\n",
    "        selfattn_mask = subsequent_mask + mask.unsqueeze(-2)\n",
    "\n",
    "        attn_mask = memory_mask.unsqueeze(-2)\n",
    "\n",
    "        # TODO: Self-Attention block\n",
    "        selfattn_out, selfattn_w = self.self_attn(x, x, x, selfattn_mask)\n",
    "        selfattn_out = self.dropout(selfattn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (1)\n",
    "        x = self.norm1(x + selfattn_out)\n",
    "\n",
    "        # TODO: Encoder-Decoder Attention block\n",
    "        attn_out, attn_w = self.encdec_attn(x, memory, memory, attn_mask)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (2)\n",
    "        x = self.norm2(x + attn_out)\n",
    "\n",
    "        # TODO: FFN block\n",
    "        ffn_out = self.ffn(x)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (3)\n",
    "        x = self.norm3(x + ffn_out)\n",
    "\n",
    "        if return_att:\n",
    "            return x, selfattn_w, attn_w\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        # Create an embedding table (T x B -> T x B x embed_dim)\n",
    "        # self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # Create the positional encoding with the class defined before\n",
    "        self.pos_enc = PositionalEncoding(embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(embed_dim, ffn_dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Add a projection layer (T x B x embed_dim -> T x B x vocab_size)\n",
    "        # self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, memory, mask=None, memory_mask=None, return_att=False):\n",
    "        #x = self.embedding(x)\n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        selfattn_ws = []\n",
    "        attn_ws = []\n",
    "        for l in self.layers:\n",
    "            if return_att:\n",
    "                x, selfattn_w, attn_w = l(\n",
    "                    x, memory, mask=mask, memory_mask=memory_mask, return_att=True\n",
    "                )\n",
    "                selfattn_ws.append(selfattn_w)\n",
    "                attn_ws.append(attn_w)\n",
    "            else:\n",
    "                x = l(\n",
    "                    x, memory, mask=mask, memory_mask=memory_mask, return_att=False\n",
    "                )\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "\n",
    "        if return_att:\n",
    "            selfattn_ws = torch.stack(selfattn_ws, dim=1)\n",
    "            attn_ws = torch.stack(attn_ws, dim=1)\n",
    "            return x, selfattn_ws, attn_ws\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder_config, decoder_config):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = TransformerEncoder(**encoder_config)\n",
    "        self.decoder = TransformerDecoder(**decoder_config)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\" Forward method\n",
    "\n",
    "        Method used at training time, when the target is known. The target tensor\n",
    "        passed to the decoder is shifted to the right (starting with BOS\n",
    "        symbol). Then, the output of the decoder starts directly with the first\n",
    "        token of the sentence.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Compute the encoder output\n",
    "        encoder_out = self.encoder(src, src_mask)\n",
    "\n",
    "        # TODO: Compute the decoder output\n",
    "        decoder_out = self.decoder(\n",
    "            x=tgt,\n",
    "            memory=encoder_out,\n",
    "            mask=tgt_mask,\n",
    "            memory_mask=src_mask\n",
    "        )\n",
    "\n",
    "        return decoder_out\n",
    "\n",
    "    def generate(self, src, src_mask=None, bos_idx=0, max_len=50):\n",
    "        \"\"\" Generate method\n",
    "\n",
    "        Method used at inference time, when the target is unknown. It\n",
    "        iteratively passes to the decoder the sequence generated so far\n",
    "        and appends the new token to the input again. It uses a Greedy\n",
    "        decoding (argmax).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Compute the encoder output\n",
    "        encoder_out = self.encoder(src, src_mask)\n",
    "\n",
    "        output = torch.LongTensor([bos_idx])\\\n",
    "                    .expand(1, encoder_out.size(1)).to(src.device)\n",
    "        for i in range(max_len):\n",
    "            # TODO: Get the new token\n",
    "            new_token = self.decoder(\n",
    "                x=output,\n",
    "                memory=encoder_out,\n",
    "                memory_mask=src_mask\n",
    "            )[-1].argmax(-1)\n",
    "\n",
    "            output = torch.cat([output, new_token.unsqueeze(0)], dim=0)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        GFSK\n",
      "2        BPSK\n",
      "3        BPSK\n",
      "4        BPSK\n",
      "5        8PSK\n",
      "         ... \n",
      "11996    BPSK\n",
      "11997    8PSK\n",
      "11998    QPSK\n",
      "11999    8PSK\n",
      "12000    QPSK\n",
      "Name: 258, Length: 12000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from seq2seq_numbers_dataset import generate_dataset_pytorch, Seq2SeqNumbersCollater\n",
    "\n",
    "TRAIN_SIZE = 1000\n",
    "\n",
    "data = pd.read_csv(\"./dataset_train_2024.csv\", header=None)\n",
    "\n",
    "label = data.iloc[1:, 258]\n",
    "\n",
    "print(label)\n",
    "\n",
    "data = data.iloc[1:, : 257]\n",
    "\n",
    "trainData = data[:TRAIN_SIZE]\n",
    "trainLabels = label[:TRAIN_SIZE]\n",
    "\n",
    "data.head\n",
    "\n",
    "collater = Seq2SeqNumbersCollater(\n",
    "    trainData,\n",
    "    trainLabels,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0         1         2         3         4         5         6    \\\n",
      "1       0.0 -0.002737 -0.003256 -0.002842 -0.003326 -0.003696 -0.002624   \n",
      "2       1.0 -0.002686 -0.003358 -0.004155 -0.005550 -0.006590 -0.007223   \n",
      "3       2.0 -0.002638 -0.002471 -0.002312 -0.002172 -0.002040 -0.002214   \n",
      "4       3.0 -0.001875 -0.002034 -0.002197 -0.002201 -0.002347 -0.002576   \n",
      "5       4.0 -0.006637 -0.006698 -0.007560 -0.007685 -0.008237 -0.007881   \n",
      "...     ...       ...       ...       ...       ...       ...       ...   \n",
      "996   995.0  0.005960  0.005648  0.005329  0.004734  0.004226  0.003602   \n",
      "997   996.0  0.003330  0.003050  0.004500  0.005088  0.005770  0.007044   \n",
      "998   997.0 -0.000866 -0.002570 -0.004254 -0.005894 -0.007288 -0.008403   \n",
      "999   998.0  0.002161  0.003685  0.005081  0.006265  0.007204  0.007833   \n",
      "1000  999.0  0.004353  0.004604  0.004521  0.004428  0.004170  0.004015   \n",
      "\n",
      "           7         8         9    ...       247       248       249  \\\n",
      "1    -0.002620 -0.001829 -0.001033  ...  0.003035  0.002601  0.002027   \n",
      "2    -0.008217 -0.007652 -0.007635  ... -0.000154  0.002098  0.003441   \n",
      "3    -0.002414 -0.002673 -0.002983  ...  0.008938  0.007760  0.006023   \n",
      "4    -0.002803 -0.002939 -0.002884  ...  0.009046  0.008281  0.007631   \n",
      "5    -0.006156 -0.006350 -0.005546  ... -0.005254 -0.007157 -0.008638   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "996   0.002776  0.001995  0.001147  ... -0.006805 -0.006651 -0.006472   \n",
      "997   0.006790  0.007132  0.007646  ...  0.005286  0.004649  0.003885   \n",
      "998  -0.009306 -0.009867 -0.010143  ... -0.007472 -0.005698 -0.003206   \n",
      "999   0.008174  0.008176  0.007942  ...  0.006020  0.006687  0.007170   \n",
      "1000  0.003646  0.003596  0.003411  ... -0.003224 -0.001940 -0.000613   \n",
      "\n",
      "           250       251       252       253       254       255       256  \n",
      "1     0.001587  0.001841  0.000575  0.001187  0.002046  0.001886  0.002628  \n",
      "2     0.005547  0.006535  0.007792  0.008272  0.008922  0.009115  0.008782  \n",
      "3     0.003708  0.000989 -0.001881 -0.004819 -0.007318 -0.009326 -0.010401  \n",
      "4     0.007210  0.007103  0.007050  0.007189  0.007495  0.007642  0.007892  \n",
      "5    -0.009310 -0.009821 -0.009019 -0.007089 -0.005824 -0.003152 -0.002219  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "996  -0.005976 -0.005619 -0.005147 -0.004462 -0.003799 -0.003172 -0.002490  \n",
      "997   0.003591  0.002326  0.001391  0.000361 -0.000675 -0.001008 -0.000936  \n",
      "998  -0.000475  0.002331  0.004923  0.007142  0.008841  0.009958  0.010485  \n",
      "999   0.007336  0.007188  0.006765  0.006229  0.007247  0.007944  0.008060  \n",
      "1000  0.000733  0.002254  0.003651  0.005167  0.006444  0.007592  0.008458  \n",
      "\n",
      "[1000 rows x 257 columns]\n",
      "Training model...\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000025FC24F5F60>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "535",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 535",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[210], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m loss_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(numbers_loader_train)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (src, tgt) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(numbers_loader_train):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(src)\n\u001b[0;32m     47\u001b[0m     src \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m src\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 535"
     ]
    }
   ],
   "source": [
    "lr = 5e-4\n",
    "batch_size = 32\n",
    "log_interval = 50\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "numbers_loader_train = DataLoader(\n",
    "    trainData,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collater,\n",
    ")\n",
    "\n",
    "print(numbers_loader_train.dataset)\n",
    "\n",
    "src_dict = trainData\n",
    "tgt_dict = trainLabels\n",
    "\n",
    "transformer_encoder_cfg = {\n",
    "    \"num_layers\": 3,\n",
    "    \"embed_dim\": 256,\n",
    "    \"ffn_dim\": 1024,\n",
    "    \"num_heads\": 4,\n",
    "    #\"vocab_size\": len(src_dict),\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "transformer_decoder_cfg = {\n",
    "    \"num_layers\": 3,\n",
    "    \"embed_dim\": 256,\n",
    "    \"ffn_dim\": 1024,\n",
    "    \"num_heads\": 4,\n",
    "    # \"vocab_size\": len(tgt_dict),\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "model = Transformer(transformer_encoder_cfg, transformer_decoder_cfg)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = F.nll_loss\n",
    "\n",
    "print(\"Training model...\")\n",
    "\n",
    "loss_avg = 0\n",
    "print(numbers_loader_train)\n",
    "for i, (src, tgt) in enumerate(numbers_loader_train):\n",
    "    print(src)\n",
    "    src = {k: v.to(device) for k, v in src.items()}\n",
    "    print(src)\n",
    "    tgt = {k: v.to(device) for k, v in tgt.items()}\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(\n",
    "        src['ids'],\n",
    "        tgt['ids'][:-1],\n",
    "        src['padding_mask'],\n",
    "        tgt['padding_mask'][:, :-1],\n",
    "    )\n",
    "\n",
    "    loss = criterion(\n",
    "        output.reshape(-1, output.size(-1)),\n",
    "        tgt['ids'][1:].flatten()\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_avg += loss.item()\n",
    "    if (i+1) % log_interval == 0:\n",
    "        loss_avg /= log_interval\n",
    "        print(f\"{i+1}/{len(numbers_loader_train)}\\tLoss: {loss_avg}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
