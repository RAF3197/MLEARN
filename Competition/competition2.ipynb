{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the multiHead attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    \"\"\" Computes the Scaled Dot-Product Attention\n",
    "\n",
    "    Args:\n",
    "        q (torch.FloatTensor):  Query Tensor   (... x T_q x d_q)\n",
    "        k (torch.FloatTensor):  Key Tensor     (... x T_k x d_k)\n",
    "        v (torch.FloatTensor):  Value Tensor   (... x T_v x d_v)\n",
    "        mask (torch.BoolTensor): Attention mask (... x T_q x T_k)\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: Result of the SDPA  (... x T_q x d_v)\n",
    "        torch.FloatTensor: Attention map       (... x T_q x T_k)\n",
    "\n",
    "    \"\"\"\n",
    "    assert q.size(-1) == k.size(-1), \"Query and Key dimensions must coincide\"\n",
    "\n",
    "    # TODO: Matrix multiplication of the queries and the keys (use torch.matmul)\n",
    "    #attn_logits =\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "\n",
    "    # TODO: Scale attn_logits (see the SDPA formula, d_k is the last dim of k)\n",
    "    #attn_logits = \n",
    "    attn_logits = attn_logits/torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))\n",
    "\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask, -float(\"inf\"))\n",
    "\n",
    "    # TODO: Compute the attention weights (see the SDPA formula, use dim=-1)\n",
    "    #attention =\n",
    "    attention = torch.softmax(attn_logits, dim=-1)\n",
    "\n",
    "    output = torch.matmul(attention, v)\n",
    "\n",
    "    return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \\\n",
    "            \"Embedding dimension must be multiple of the number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.proj_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization\n",
    "        nn.init.xavier_uniform_(self.proj_q.weight)\n",
    "        nn.init.xavier_uniform_(self.proj_k.weight)\n",
    "        nn.init.xavier_uniform_(self.proj_v.weight)\n",
    "        nn.init.xavier_uniform_(self.proj_o.weight)\n",
    "        self.proj_q.bias.data.fill_(0)\n",
    "        self.proj_k.bias.data.fill_(0)\n",
    "        self.proj_v.bias.data.fill_(0)\n",
    "        self.proj_o.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(1)\n",
    "\n",
    "        q = self.proj_q(q)\n",
    "        k = self.proj_k(k)\n",
    "        v = self.proj_v(v)\n",
    "\n",
    "        # TODO: Split the tensors into multiple heads\n",
    "        #  T x B x embed_dim -> T x B x num_heads x head_dim\n",
    "        q = q.reshape(q.size(0), batch_size, self.num_heads, self.head_dim)\n",
    "        k = k.reshape(k.size(0), batch_size, self.num_heads, self.head_dim)\n",
    "        v = v.reshape(v.size(0), batch_size, self.num_heads, self.head_dim)\n",
    "\n",
    "        # The last two dimensions must be sequence length and the head dimension,\n",
    "        # to make it work with the scaled dot-product function.\n",
    "        # TODO: Rearrange the dimensions\n",
    "        # T x B x num_heads x head_dim -> B x num_heads x T x head_dim\n",
    "        q = q.permute(1, 2, 0, 3)\n",
    "        k = k.permute(1, 2, 0, 3)\n",
    "        v = v.permute(1, 2, 0, 3)\n",
    "\n",
    "        # Apply the same mask to all the heads\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # TODO: Call the scaled dot-product function (remember to pass the mask!)\n",
    "        output_heads, attn_w = scaled_dot_product(q, k, v, mask)\n",
    "\n",
    "        # B x num_heads x T x head_dim -> T x B x num_heads x head_dim\n",
    "        output_heads = output_heads.permute(2, 0, 1, 3)\n",
    "\n",
    "        # T x B x num_heads x head_dim -> T x B x embed_dim\n",
    "        output_cat = output_heads.reshape(-1, batch_size, self.embed_dim)\n",
    "        output = self.proj_o(output_cat)\n",
    "\n",
    "        return output, attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimensionality\n",
    "            max_len (int): Maximum length of a sequence to expect\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create matrix of (T x embed_dim) representing the positional encoding\n",
    "        # for max_len inputs\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimensionality (input, output & self-attention)\n",
    "            ffn_dim (int): Inner dimensionality in the FFN\n",
    "            num_heads (int): Number of heads of the multi-head attention block\n",
    "            dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiheadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(ffn_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None, return_att=False):\n",
    "        src_len, batch_size, _ = x.shape\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(x.shape[1], x.shape[0]).bool().to(x.device)\n",
    "\n",
    "        selfattn_mask = mask.unsqueeze(-2)\n",
    "\n",
    "        # TODO: Self-Attention block\n",
    "        selfattn_out, selfattn_w = self.self_attn(x, x, x, selfattn_mask)\n",
    "        selfattn_out = self.dropout(selfattn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (1)\n",
    "        x = self.norm1(x + selfattn_out)\n",
    "\n",
    "        # TODO: FFN block\n",
    "        ffn_out = self.ffn(x)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (2)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "\n",
    "        if return_att:\n",
    "            return x, selfattn_w\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        # Create an embedding table (T x B -> T x B x embed_dim)\n",
    "        # self.embedding = nn.Embedding(, embed_dim)\n",
    "\n",
    "        # Create the positional encoding with the class defined before\n",
    "        self.pos_enc = PositionalEncoding(embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, ffn_dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None, return_att=False):\n",
    "        #x = self.embedding(x)\n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        selfattn_ws = []\n",
    "        for l in self.layers:\n",
    "            if return_att:\n",
    "                x, selfattn_w = l(x, mask=mask, return_att=True)\n",
    "                selfattn_ws.append(selfattn_w)\n",
    "            else:\n",
    "                x = l(x, mask=mask, return_att=False)\n",
    "\n",
    "        if return_att:\n",
    "            selfattn_ws = torch.stack(selfattn_ws, dim=1)\n",
    "            return x, selfattn_ws\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimensionality (input, output & self-attention)\n",
    "            ffn_dim (int): Inner dimensionality in the FFN\n",
    "            num_heads (int): Number of heads of the multi-head attention block\n",
    "            dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiheadAttention(embed_dim, num_heads)\n",
    "        self.encdec_attn = MultiheadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(ffn_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, memory, mask=None, memory_mask=None, return_att=False):\n",
    "        tgt_len, batch_size, _ = x.shape\n",
    "        src_len, _, _ = memory.shape\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(x.shape[1], x.shape[0])\n",
    "            mask = mask.bool().to(x.device)\n",
    "        if memory_mask is None:\n",
    "            memory_mask = torch.zeros(memory.shape[1], memory.shape[0])\n",
    "            memory_mask = memory_mask.bool().to(memory.device)\n",
    "\n",
    "\n",
    "        subsequent_mask = torch.triu(torch.ones(batch_size, tgt_len, tgt_len), 1)\n",
    "        subsequent_mask = subsequent_mask.bool().to(mask.device)\n",
    "        selfattn_mask = subsequent_mask + mask.unsqueeze(-2)\n",
    "\n",
    "        attn_mask = memory_mask.unsqueeze(-2)\n",
    "\n",
    "        # TODO: Self-Attention block\n",
    "        selfattn_out, selfattn_w = self.self_attn(x, x, x, selfattn_mask)\n",
    "        selfattn_out = self.dropout(selfattn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (1)\n",
    "        x = self.norm1(x + selfattn_out)\n",
    "\n",
    "        # TODO: Encoder-Decoder Attention block\n",
    "        attn_out, attn_w = self.encdec_attn(x, memory, memory, attn_mask)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (2)\n",
    "        x = self.norm2(x + attn_out)\n",
    "\n",
    "        # TODO: FFN block\n",
    "        ffn_out = self.ffn(x)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (3)\n",
    "        x = self.norm3(x + ffn_out)\n",
    "\n",
    "        if return_att:\n",
    "            return x, selfattn_w, attn_w\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        # Create an embedding table (T x B -> T x B x embed_dim)\n",
    "        # self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # Create the positional encoding with the class defined before\n",
    "        self.pos_enc = PositionalEncoding(embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(embed_dim, ffn_dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Add a projection layer (T x B x embed_dim -> T x B x vocab_size)\n",
    "        # self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, memory, mask=None, memory_mask=None, return_att=False):\n",
    "        #x = self.embedding(x)\n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        selfattn_ws = []\n",
    "        attn_ws = []\n",
    "        for l in self.layers:\n",
    "            if return_att:\n",
    "                x, selfattn_w, attn_w = l(\n",
    "                    x, memory, mask=mask, memory_mask=memory_mask, return_att=True\n",
    "                )\n",
    "                selfattn_ws.append(selfattn_w)\n",
    "                attn_ws.append(attn_w)\n",
    "            else:\n",
    "                x = l(\n",
    "                    x, memory, mask=mask, memory_mask=memory_mask, return_att=False\n",
    "                )\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "\n",
    "        if return_att:\n",
    "            selfattn_ws = torch.stack(selfattn_ws, dim=1)\n",
    "            attn_ws = torch.stack(attn_ws, dim=1)\n",
    "            return x, selfattn_ws, attn_ws\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder_config, decoder_config):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = TransformerEncoder(**encoder_config)\n",
    "        self.decoder = TransformerDecoder(**decoder_config)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\" Forward method\n",
    "\n",
    "        Method used at training time, when the target is known. The target tensor\n",
    "        passed to the decoder is shifted to the right (starting with BOS\n",
    "        symbol). Then, the output of the decoder starts directly with the first\n",
    "        token of the sentence.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Compute the encoder output\n",
    "        encoder_out = self.encoder(src, src_mask)\n",
    "\n",
    "        # TODO: Compute the decoder output\n",
    "        decoder_out = self.decoder(\n",
    "            x=tgt,\n",
    "            memory=encoder_out,\n",
    "            mask=tgt_mask,\n",
    "            memory_mask=src_mask\n",
    "        )\n",
    "\n",
    "        return decoder_out\n",
    "\n",
    "    def generate(self, src, src_mask=None, bos_idx=0, max_len=50):\n",
    "        \"\"\" Generate method\n",
    "\n",
    "        Method used at inference time, when the target is unknown. It\n",
    "        iteratively passes to the decoder the sequence generated so far\n",
    "        and appends the new token to the input again. It uses a Greedy\n",
    "        decoding (argmax).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Compute the encoder output\n",
    "        encoder_out = self.encoder(src, src_mask)\n",
    "\n",
    "        output = torch.LongTensor([bos_idx])\\\n",
    "                    .expand(1, encoder_out.size(1)).to(src.device)\n",
    "        for i in range(max_len):\n",
    "            # TODO: Get the new token\n",
    "            new_token = self.decoder(\n",
    "                x=output,\n",
    "                memory=encoder_out,\n",
    "                memory_mask=src_mask\n",
    "            )[-1].argmax(-1)\n",
    "\n",
    "            output = torch.cat([output, new_token.unsqueeze(0)], dim=0)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        0         1         2         3         4         5         6    \\\n",
       "1      0.0 -0.002737 -0.003256 -0.002842 -0.003326 -0.003696 -0.002624   \n",
       "2      1.0 -0.002686 -0.003358 -0.004155 -0.005550 -0.006590 -0.007223   \n",
       "3      2.0 -0.002638 -0.002471 -0.002312 -0.002172 -0.002040 -0.002214   \n",
       "4      3.0 -0.001875 -0.002034 -0.002197 -0.002201 -0.002347 -0.002576   \n",
       "5      4.0 -0.006637 -0.006698 -0.007560 -0.007685 -0.008237 -0.007881   \n",
       "..     ...       ...       ...       ...       ...       ...       ...   \n",
       "995  994.0 -0.002752 -0.002079 -0.001220 -0.000595  0.000197  0.000827   \n",
       "996  995.0  0.005960  0.005648  0.005329  0.004734  0.004226  0.003602   \n",
       "997  996.0  0.003330  0.003050  0.004500  0.005088  0.005770  0.007044   \n",
       "998  997.0 -0.000866 -0.002570 -0.004254 -0.005894 -0.007288 -0.008403   \n",
       "999  998.0  0.002161  0.003685  0.005081  0.006265  0.007204  0.007833   \n",
       "\n",
       "          7         8         9    ...       247       248       249  \\\n",
       "1   -0.002620 -0.001829 -0.001033  ...  0.003035  0.002601  0.002027   \n",
       "2   -0.008217 -0.007652 -0.007635  ... -0.000154  0.002098  0.003441   \n",
       "3   -0.002414 -0.002673 -0.002983  ...  0.008938  0.007760  0.006023   \n",
       "4   -0.002803 -0.002939 -0.002884  ...  0.009046  0.008281  0.007631   \n",
       "5   -0.006156 -0.006350 -0.005546  ... -0.005254 -0.007157 -0.008638   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.001531  0.002319  0.003068  ...  0.007287  0.007262  0.007102   \n",
       "996  0.002776  0.001995  0.001147  ... -0.006805 -0.006651 -0.006472   \n",
       "997  0.006790  0.007132  0.007646  ...  0.005286  0.004649  0.003885   \n",
       "998 -0.009306 -0.009867 -0.010143  ... -0.007472 -0.005698 -0.003206   \n",
       "999  0.008174  0.008176  0.007942  ...  0.006020  0.006687  0.007170   \n",
       "\n",
       "          250       251       252       253       254       255       256  \n",
       "1    0.001587  0.001841  0.000575  0.001187  0.002046  0.001886  0.002628  \n",
       "2    0.005547  0.006535  0.007792  0.008272  0.008922  0.009115  0.008782  \n",
       "3    0.003708  0.000989 -0.001881 -0.004819 -0.007318 -0.009326 -0.010401  \n",
       "4    0.007210  0.007103  0.007050  0.007189  0.007495  0.007642  0.007892  \n",
       "5   -0.009310 -0.009821 -0.009019 -0.007089 -0.005824 -0.003152 -0.002219  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995  0.007043  0.007077  0.007282  0.007349  0.007484  0.007647  0.008022  \n",
       "996 -0.005976 -0.005619 -0.005147 -0.004462 -0.003799 -0.003172 -0.002490  \n",
       "997  0.003591  0.002326  0.001391  0.000361 -0.000675 -0.001008 -0.000936  \n",
       "998 -0.000475  0.002331  0.004923  0.007142  0.008841  0.009958  0.010485  \n",
       "999  0.007336  0.007188  0.006765  0.006229  0.007247  0.007944  0.008060  \n",
       "\n",
       "[999 rows x 257 columns]>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SET_SIZE = 1000\n",
    "\n",
    "data = pd.read_csv(\"./data/dataset_train_2024.csv\", header=None)\n",
    "data = data.iloc[1:TRAIN_SET_SIZE, : ]\n",
    "\n",
    "trainData = data.iloc[:, 258]\n",
    "trainLabels = data.iloc[:, : 257]\n",
    "\n",
    "trainData.head\n",
    "trainLabels.head\n",
    "\n",
    "\n",
    "\n",
    "# def custom_collater(batch):\n",
    "#     \"\"\"\n",
    "#     Custom collater function for batching sequence data.\n",
    "\n",
    "#     Args:\n",
    "#         batch (list of tuples): Each tuple contains (data, label).\n",
    "        \n",
    "#     Returns:\n",
    "#         dict: Batched input data with padding mask.\n",
    "#         dict: Batched target labels with padding mask.\n",
    "#     \"\"\"\n",
    "#     # Extract sequences and labels from the batch\n",
    "#     sequences, labels = zip(*batch)\n",
    "    \n",
    "#     # Convert to PyTorch tensors\n",
    "#     sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "#     labels = [torch.tensor(lbl, dtype=torch.long) for lbl in labels]\n",
    "    \n",
    "#     # Determine max sequence length for padding\n",
    "#     max_seq_len = max([len(seq) for seq in sequences])\n",
    "#     max_label_len = max([len(lbl) for lbl in labels])\n",
    "    \n",
    "#     # Pad sequences and labels\n",
    "#     padded_sequences = torch.zeros(len(sequences), max_seq_len, dtype=torch.long)\n",
    "#     sequence_padding_mask = torch.ones(len(sequences), max_seq_len, dtype=torch.bool)\n",
    "    \n",
    "#     for i, seq in enumerate(sequences):\n",
    "#         padded_sequences[i, :len(seq)] = seq\n",
    "#         sequence_padding_mask[i, :len(seq)] = False\n",
    "    \n",
    "#     padded_labels = torch.zeros(len(labels), max_label_len, dtype=torch.long)\n",
    "#     label_padding_mask = torch.ones(len(labels), max_label_len, dtype=torch.bool)\n",
    "    \n",
    "#     for i, lbl in enumerate(labels):\n",
    "#         padded_labels[i, :len(lbl)] = lbl\n",
    "#         label_padding_mask[i, :len(lbl)] = False\n",
    "\n",
    "#     # Package into dictionaries\n",
    "#     src = {\n",
    "#         \"ids\": padded_sequences,\n",
    "#         \"padding_mask\": sequence_padding_mask,\n",
    "#     }\n",
    "#     tgt = {\n",
    "#         \"ids\": padded_labels,\n",
    "#         \"padding_mask\": label_padding_mask,\n",
    "#     }\n",
    "\n",
    "#     return src, tgt\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_collater' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 10\u001b[0m\n\u001b[0;32m      3\u001b[0m log_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m numbers_loader_train \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(trainData\u001b[38;5;241m.\u001b[39mvalues, trainLabels\u001b[38;5;241m.\u001b[39mvalues)),\n\u001b[0;32m      8\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m      9\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m---> 10\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39m\u001b[43mcustom_collater\u001b[49m,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m transformer_encoder_cfg \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     19\u001b[0m }\n\u001b[0;32m     20\u001b[0m transformer_decoder_cfg \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     26\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'custom_collater' is not defined"
     ]
    }
   ],
   "source": [
    "lr = 5e-4\n",
    "batch_size = 32\n",
    "log_interval = 50\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "numbers_loader_train = DataLoader(\n",
    "    list(zip(trainData.values, trainLabels.values)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collater,\n",
    ")\n",
    "\n",
    "transformer_encoder_cfg = {\n",
    "    \"num_layers\": 3,\n",
    "    \"embed_dim\": 256,\n",
    "    \"ffn_dim\": 1024,\n",
    "    \"num_heads\": 4,\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "transformer_decoder_cfg = {\n",
    "    \"num_layers\": 3,\n",
    "    \"embed_dim\": 256,\n",
    "    \"ffn_dim\": 1024,\n",
    "    \"num_heads\": 4,\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "model = Transformer(transformer_encoder_cfg, transformer_decoder_cfg)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = F.nll_loss\n",
    "\n",
    "print(\"Training model...\")\n",
    "\n",
    "loss_avg = 0\n",
    "print(numbers_loader_train)\n",
    "for i, (src, tgt) in enumerate(numbers_loader_train):\n",
    "    print(src)\n",
    "    src = {k: v.to(device) for k, v in src.items()}\n",
    "    print(src)\n",
    "    tgt = {k: v.to(device) for k, v in tgt.items()}\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(\n",
    "        src['ids'],\n",
    "        tgt['ids'][:-1],\n",
    "        src['padding_mask'],\n",
    "        tgt['padding_mask'][:, :-1],\n",
    "    )\n",
    "\n",
    "    loss = criterion(\n",
    "        output.reshape(-1, output.size(-1)),\n",
    "        tgt['ids'][1:].flatten()\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_avg += loss.item()\n",
    "    if (i+1) % log_interval == 0:\n",
    "        loss_avg /= log_interval\n",
    "        print(f\"{i+1}/{len(numbers_loader_train)}\\tLoss: {loss_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        # Load data from CSV\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Extract features\n",
    "        self.sequences_1 = data.iloc[:, 1:129].values  # Columns 1-128 (1-based indexing)\n",
    "        self.sequences_2 = data.iloc[:, 129:257].values  # Columns 129-256\n",
    "        self.extra_feature = data.iloc[:, 257].values  # Column 257\n",
    "        self.features = torch.tensor(\n",
    "            np.hstack([self.sequences_1, self.sequences_2, self.extra_feature.reshape(-1, 1)]),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        # Encode labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = torch.tensor(self.label_encoder.fit_transform(data.iloc[:, -1]), dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "    \n",
    "    def inverseTransform(self, array):\n",
    "        return self.label_encoder.inverse_transform(array)\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, ff_dim, num_classes, dropout=0.1):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, input_dim, embed_dim))  # Match input_dim\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, feature_dim = x.size()\n",
    "        if feature_dim != self.positional_encoding.size(1):\n",
    "            raise ValueError(f\"Feature dimension mismatch: Expected {self.positional_encoding.size(1)}, got {feature_dim}\")\n",
    "\n",
    "        x = self.embedding(x).unsqueeze(1)  # (batch_size, 1, embed_dim)\n",
    "        x += self.positional_encoding[:, :1, :]  # Add positional encoding\n",
    "        x = x.permute(1, 0, 2)  # (seq_len=1, batch_size, embed_dim)\n",
    "        x = self.transformer_encoder(x)  # (seq_len=1, batch_size, embed_dim)\n",
    "        x = x.mean(dim=0)  # (batch_size, embed_dim)\n",
    "        return self.fc(x)  # (batch_size, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "csv_path = \"data/dataset_train_2024.csv\"  # Path to the dataset CSV file\n",
    "batch_size = 64\n",
    "epochs = 1500\n",
    "learning_rate = 5e-4\n",
    "input_dim = 257  # 128+128+1\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "num_layers = 1\n",
    "ff_dim = 4 * embed_dim\n",
    "num_classes = 5  # Adjust based on the dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = CustomDataset(csv_path)\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size  # 20% for testing\n",
    "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = TransformerClassifier(input_dim, embed_dim, num_heads, num_layers, ff_dim, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "#scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#    optimizer, max_lr=5e-4, steps_per_epoch=len(train_loader), epochs=epochs\n",
    "#)\n",
    "\n",
    "#from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "#scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/1500, Loss: 1.6240\n",
      "Epoch 2/1500, Loss: 1.6189\n",
      "Epoch 3/1500, Loss: 1.6162\n",
      "Epoch 4/1500, Loss: 1.6138\n",
      "Epoch 5/1500, Loss: 1.6142\n",
      "Epoch 6/1500, Loss: 1.6125\n",
      "Epoch 7/1500, Loss: 1.6146\n",
      "Epoch 8/1500, Loss: 1.6141\n",
      "Epoch 9/1500, Loss: 1.6123\n",
      "Epoch 10/1500, Loss: 1.6124\n",
      "Epoch 11/1500, Loss: 1.6138\n",
      "Epoch 12/1500, Loss: 1.6127\n",
      "Epoch 13/1500, Loss: 1.6112\n",
      "Epoch 14/1500, Loss: 1.6112\n",
      "Epoch 15/1500, Loss: 1.6107\n",
      "Epoch 16/1500, Loss: 1.6108\n",
      "Epoch 17/1500, Loss: 1.6097\n",
      "Epoch 18/1500, Loss: 1.6108\n",
      "Epoch 19/1500, Loss: 1.6098\n",
      "Epoch 20/1500, Loss: 1.6092\n",
      "Epoch 21/1500, Loss: 1.6100\n",
      "Epoch 22/1500, Loss: 1.6092\n",
      "Epoch 23/1500, Loss: 1.6090\n",
      "Epoch 24/1500, Loss: 1.6091\n",
      "Epoch 25/1500, Loss: 1.6089\n",
      "Epoch 26/1500, Loss: 1.6092\n",
      "Epoch 27/1500, Loss: 1.6077\n",
      "Epoch 28/1500, Loss: 1.6099\n",
      "Epoch 29/1500, Loss: 1.6080\n",
      "Epoch 30/1500, Loss: 1.6085\n",
      "Epoch 31/1500, Loss: 1.6073\n",
      "Epoch 32/1500, Loss: 1.6076\n",
      "Epoch 33/1500, Loss: 1.6071\n",
      "Epoch 34/1500, Loss: 1.6026\n",
      "Epoch 35/1500, Loss: 1.5721\n",
      "Epoch 36/1500, Loss: 1.5461\n",
      "Epoch 37/1500, Loss: 1.5125\n",
      "Epoch 38/1500, Loss: 1.4829\n",
      "Epoch 39/1500, Loss: 1.4638\n",
      "Epoch 40/1500, Loss: 1.4568\n",
      "Epoch 41/1500, Loss: 1.4403\n",
      "Epoch 42/1500, Loss: 1.4186\n",
      "Epoch 43/1500, Loss: 1.3845\n",
      "Epoch 44/1500, Loss: 1.3613\n",
      "Epoch 45/1500, Loss: 1.3482\n",
      "Epoch 46/1500, Loss: 1.3337\n",
      "Epoch 47/1500, Loss: 1.3143\n",
      "Epoch 48/1500, Loss: 1.2964\n",
      "Epoch 49/1500, Loss: 1.2737\n",
      "Epoch 50/1500, Loss: 1.2544\n",
      "Epoch 51/1500, Loss: 1.2211\n",
      "Epoch 52/1500, Loss: 1.2139\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "print(\"Training the model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    #scheduler.step(total_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n",
      "F1 Score: 0.3699\n",
      "Test Accuracy: 0.3883\n"
     ]
    }
   ],
   "source": [
    "# Testing Loop\n",
    "print(\"Testing the model...\")\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        outputs = model(features)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')  # or 'macro', 'micro', depending on your use case\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'predictions_with_indices.csv'\n"
     ]
    }
   ],
   "source": [
    "#Using the model for prediction with the evaluation dataset\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define the dataset class\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.data.iloc[idx].values.astype('float32')  # Adjust for your data type\n",
    "        if self.transform:\n",
    "            inputs = self.transform(inputs)\n",
    "        return inputs\n",
    "\n",
    "# Load the unlabeled dataset\n",
    "csv_path = \"data/dataset_test_no_label_2024.csv\"  # Path to the dataset CSV file\n",
    "unlabeled_df = pd.read_csv(csv_path)  # Update the filename\n",
    "unlabeled_df = unlabeled_df.drop(unlabeled_df.columns[0], axis=1)\n",
    "unlabeled_dataset = UnlabeledDataset(unlabeled_df)\n",
    "unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Store predictions and indices\n",
    "predictions = []\n",
    "indices = []\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    for idx, inputs in enumerate(unlabeled_dataloader):\n",
    "        inputs = inputs.to(device)  # Send inputs to the same device as the model\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)  # Get predicted class\n",
    "        \n",
    "        # Save predictions and indices\n",
    "        start_idx = idx * unlabeled_dataloader.batch_size\n",
    "        batch_indices = list(range(start_idx, start_idx + len(inputs)))  # Adjusting the index properly\n",
    "        indices.extend(batch_indices)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Create a DataFrame with indices and predictions\n",
    "output_df = pd.DataFrame({\"ID\": indices, \"MODULATION\": dataset.inverseTransform(predictions)})\n",
    "\n",
    "# Save to a CSV file\n",
    "output_df.to_csv(\"predictions_with_indices.csv\", index=False)\n",
    "\n",
    "print(\"Predictions saved to 'predictions_with_indices.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
