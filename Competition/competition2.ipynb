{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the multiHead attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    \"\"\" Computes the Scaled Dot-Product Attention\n",
    "\n",
    "    Args:\n",
    "        q (torch.FloatTensor):  Query Tensor   (... x T_q x d_q)\n",
    "        k (torch.FloatTensor):  Key Tensor     (... x T_k x d_k)\n",
    "        v (torch.FloatTensor):  Value Tensor   (... x T_v x d_v)\n",
    "        mask (torch.BoolTensor): Attention mask (... x T_q x T_k)\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: Result of the SDPA  (... x T_q x d_v)\n",
    "        torch.FloatTensor: Attention map       (... x T_q x T_k)\n",
    "\n",
    "    \"\"\"\n",
    "    assert q.size(-1) == k.size(-1), \"Query and Key dimensions must coincide\"\n",
    "\n",
    "    # TODO: Matrix multiplication of the queries and the keys (use torch.matmul)\n",
    "    #attn_logits =\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "\n",
    "    # TODO: Scale attn_logits (see the SDPA formula, d_k is the last dim of k)\n",
    "    #attn_logits = \n",
    "    attn_logits = attn_logits/torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))\n",
    "\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask, -float(\"inf\"))\n",
    "\n",
    "    # TODO: Compute the attention weights (see the SDPA formula, use dim=-1)\n",
    "    #attention =\n",
    "    attention = torch.softmax(attn_logits, dim=-1)\n",
    "\n",
    "    output = torch.matmul(attention, v)\n",
    "\n",
    "    return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \\\n",
    "            \"Embedding dimension must be multiple of the number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.proj_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization\n",
    "        nn.init.xavier_uniform_(self.proj_q.weight)\n",
    "        nn.init.xavier_uniform_(self.proj_k.weight)\n",
    "        nn.init.xavier_uniform_(self.proj_v.weight)\n",
    "        nn.init.xavier_uniform_(self.proj_o.weight)\n",
    "        self.proj_q.bias.data.fill_(0)\n",
    "        self.proj_k.bias.data.fill_(0)\n",
    "        self.proj_v.bias.data.fill_(0)\n",
    "        self.proj_o.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(1)\n",
    "\n",
    "        q = self.proj_q(q)\n",
    "        k = self.proj_k(k)\n",
    "        v = self.proj_v(v)\n",
    "\n",
    "        # TODO: Split the tensors into multiple heads\n",
    "        #  T x B x embed_dim -> T x B x num_heads x head_dim\n",
    "        q = q.reshape(q.size(0), batch_size, self.num_heads, self.head_dim)\n",
    "        k = k.reshape(k.size(0), batch_size, self.num_heads, self.head_dim)\n",
    "        v = v.reshape(v.size(0), batch_size, self.num_heads, self.head_dim)\n",
    "\n",
    "        # The last two dimensions must be sequence length and the head dimension,\n",
    "        # to make it work with the scaled dot-product function.\n",
    "        # TODO: Rearrange the dimensions\n",
    "        # T x B x num_heads x head_dim -> B x num_heads x T x head_dim\n",
    "        q = q.permute(1, 2, 0, 3)\n",
    "        k = k.permute(1, 2, 0, 3)\n",
    "        v = v.permute(1, 2, 0, 3)\n",
    "\n",
    "        # Apply the same mask to all the heads\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # TODO: Call the scaled dot-product function (remember to pass the mask!)\n",
    "        output_heads, attn_w = scaled_dot_product(q, k, v, mask)\n",
    "\n",
    "        # B x num_heads x T x head_dim -> T x B x num_heads x head_dim\n",
    "        output_heads = output_heads.permute(2, 0, 1, 3)\n",
    "\n",
    "        # T x B x num_heads x head_dim -> T x B x embed_dim\n",
    "        output_cat = output_heads.reshape(-1, batch_size, self.embed_dim)\n",
    "        output = self.proj_o(output_cat)\n",
    "\n",
    "        return output, attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimensionality\n",
    "            max_len (int): Maximum length of a sequence to expect\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create matrix of (T x embed_dim) representing the positional encoding\n",
    "        # for max_len inputs\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimensionality (input, output & self-attention)\n",
    "            ffn_dim (int): Inner dimensionality in the FFN\n",
    "            num_heads (int): Number of heads of the multi-head attention block\n",
    "            dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiheadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(ffn_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None, return_att=False):\n",
    "        src_len, batch_size, _ = x.shape\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(x.shape[1], x.shape[0]).bool().to(x.device)\n",
    "\n",
    "        selfattn_mask = mask.unsqueeze(-2)\n",
    "\n",
    "        # TODO: Self-Attention block\n",
    "        selfattn_out, selfattn_w = self.self_attn(x, x, x, selfattn_mask)\n",
    "        selfattn_out = self.dropout(selfattn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (1)\n",
    "        x = self.norm1(x + selfattn_out)\n",
    "\n",
    "        # TODO: FFN block\n",
    "        ffn_out = self.ffn(x)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (2)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "\n",
    "        if return_att:\n",
    "            return x, selfattn_w\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        # Create an embedding table (T x B -> T x B x embed_dim)\n",
    "        # self.embedding = nn.Embedding(, embed_dim)\n",
    "\n",
    "        # Create the positional encoding with the class defined before\n",
    "        self.pos_enc = PositionalEncoding(embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, ffn_dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None, return_att=False):\n",
    "        #x = self.embedding(x)\n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        selfattn_ws = []\n",
    "        for l in self.layers:\n",
    "            if return_att:\n",
    "                x, selfattn_w = l(x, mask=mask, return_att=True)\n",
    "                selfattn_ws.append(selfattn_w)\n",
    "            else:\n",
    "                x = l(x, mask=mask, return_att=False)\n",
    "\n",
    "        if return_att:\n",
    "            selfattn_ws = torch.stack(selfattn_ws, dim=1)\n",
    "            return x, selfattn_ws\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimensionality (input, output & self-attention)\n",
    "            ffn_dim (int): Inner dimensionality in the FFN\n",
    "            num_heads (int): Number of heads of the multi-head attention block\n",
    "            dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiheadAttention(embed_dim, num_heads)\n",
    "        self.encdec_attn = MultiheadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(ffn_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, memory, mask=None, memory_mask=None, return_att=False):\n",
    "        tgt_len, batch_size, _ = x.shape\n",
    "        src_len, _, _ = memory.shape\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(x.shape[1], x.shape[0])\n",
    "            mask = mask.bool().to(x.device)\n",
    "        if memory_mask is None:\n",
    "            memory_mask = torch.zeros(memory.shape[1], memory.shape[0])\n",
    "            memory_mask = memory_mask.bool().to(memory.device)\n",
    "\n",
    "\n",
    "        subsequent_mask = torch.triu(torch.ones(batch_size, tgt_len, tgt_len), 1)\n",
    "        subsequent_mask = subsequent_mask.bool().to(mask.device)\n",
    "        selfattn_mask = subsequent_mask + mask.unsqueeze(-2)\n",
    "\n",
    "        attn_mask = memory_mask.unsqueeze(-2)\n",
    "\n",
    "        # TODO: Self-Attention block\n",
    "        selfattn_out, selfattn_w = self.self_attn(x, x, x, selfattn_mask)\n",
    "        selfattn_out = self.dropout(selfattn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (1)\n",
    "        x = self.norm1(x + selfattn_out)\n",
    "\n",
    "        # TODO: Encoder-Decoder Attention block\n",
    "        attn_out, attn_w = self.encdec_attn(x, memory, memory, attn_mask)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (2)\n",
    "        x = self.norm2(x + attn_out)\n",
    "\n",
    "        # TODO: FFN block\n",
    "        ffn_out = self.ffn(x)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (3)\n",
    "        x = self.norm3(x + ffn_out)\n",
    "\n",
    "        if return_att:\n",
    "            return x, selfattn_w, attn_w\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        # Create an embedding table (T x B -> T x B x embed_dim)\n",
    "        # self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # Create the positional encoding with the class defined before\n",
    "        self.pos_enc = PositionalEncoding(embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(embed_dim, ffn_dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Add a projection layer (T x B x embed_dim -> T x B x vocab_size)\n",
    "        # self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, memory, mask=None, memory_mask=None, return_att=False):\n",
    "        #x = self.embedding(x)\n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        selfattn_ws = []\n",
    "        attn_ws = []\n",
    "        for l in self.layers:\n",
    "            if return_att:\n",
    "                x, selfattn_w, attn_w = l(\n",
    "                    x, memory, mask=mask, memory_mask=memory_mask, return_att=True\n",
    "                )\n",
    "                selfattn_ws.append(selfattn_w)\n",
    "                attn_ws.append(attn_w)\n",
    "            else:\n",
    "                x = l(\n",
    "                    x, memory, mask=mask, memory_mask=memory_mask, return_att=False\n",
    "                )\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "\n",
    "        if return_att:\n",
    "            selfattn_ws = torch.stack(selfattn_ws, dim=1)\n",
    "            attn_ws = torch.stack(attn_ws, dim=1)\n",
    "            return x, selfattn_ws, attn_ws\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder_config, decoder_config):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = TransformerEncoder(**encoder_config)\n",
    "        self.decoder = TransformerDecoder(**decoder_config)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\" Forward method\n",
    "\n",
    "        Method used at training time, when the target is known. The target tensor\n",
    "        passed to the decoder is shifted to the right (starting with BOS\n",
    "        symbol). Then, the output of the decoder starts directly with the first\n",
    "        token of the sentence.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Compute the encoder output\n",
    "        encoder_out = self.encoder(src, src_mask)\n",
    "\n",
    "        # TODO: Compute the decoder output\n",
    "        decoder_out = self.decoder(\n",
    "            x=tgt,\n",
    "            memory=encoder_out,\n",
    "            mask=tgt_mask,\n",
    "            memory_mask=src_mask\n",
    "        )\n",
    "\n",
    "        return decoder_out\n",
    "\n",
    "    def generate(self, src, src_mask=None, bos_idx=0, max_len=50):\n",
    "        \"\"\" Generate method\n",
    "\n",
    "        Method used at inference time, when the target is unknown. It\n",
    "        iteratively passes to the decoder the sequence generated so far\n",
    "        and appends the new token to the input again. It uses a Greedy\n",
    "        decoding (argmax).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Compute the encoder output\n",
    "        encoder_out = self.encoder(src, src_mask)\n",
    "\n",
    "        output = torch.LongTensor([bos_idx])\\\n",
    "                    .expand(1, encoder_out.size(1)).to(src.device)\n",
    "        for i in range(max_len):\n",
    "            # TODO: Get the new token\n",
    "            new_token = self.decoder(\n",
    "                x=output,\n",
    "                memory=encoder_out,\n",
    "                memory_mask=src_mask\n",
    "            )[-1].argmax(-1)\n",
    "\n",
    "            output = torch.cat([output, new_token.unsqueeze(0)], dim=0)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        0         1         2         3         4         5         6    \\\n",
       "1      0.0 -0.002737 -0.003256 -0.002842 -0.003326 -0.003696 -0.002624   \n",
       "2      1.0 -0.002686 -0.003358 -0.004155 -0.005550 -0.006590 -0.007223   \n",
       "3      2.0 -0.002638 -0.002471 -0.002312 -0.002172 -0.002040 -0.002214   \n",
       "4      3.0 -0.001875 -0.002034 -0.002197 -0.002201 -0.002347 -0.002576   \n",
       "5      4.0 -0.006637 -0.006698 -0.007560 -0.007685 -0.008237 -0.007881   \n",
       "..     ...       ...       ...       ...       ...       ...       ...   \n",
       "995  994.0 -0.002752 -0.002079 -0.001220 -0.000595  0.000197  0.000827   \n",
       "996  995.0  0.005960  0.005648  0.005329  0.004734  0.004226  0.003602   \n",
       "997  996.0  0.003330  0.003050  0.004500  0.005088  0.005770  0.007044   \n",
       "998  997.0 -0.000866 -0.002570 -0.004254 -0.005894 -0.007288 -0.008403   \n",
       "999  998.0  0.002161  0.003685  0.005081  0.006265  0.007204  0.007833   \n",
       "\n",
       "          7         8         9    ...       247       248       249  \\\n",
       "1   -0.002620 -0.001829 -0.001033  ...  0.003035  0.002601  0.002027   \n",
       "2   -0.008217 -0.007652 -0.007635  ... -0.000154  0.002098  0.003441   \n",
       "3   -0.002414 -0.002673 -0.002983  ...  0.008938  0.007760  0.006023   \n",
       "4   -0.002803 -0.002939 -0.002884  ...  0.009046  0.008281  0.007631   \n",
       "5   -0.006156 -0.006350 -0.005546  ... -0.005254 -0.007157 -0.008638   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.001531  0.002319  0.003068  ...  0.007287  0.007262  0.007102   \n",
       "996  0.002776  0.001995  0.001147  ... -0.006805 -0.006651 -0.006472   \n",
       "997  0.006790  0.007132  0.007646  ...  0.005286  0.004649  0.003885   \n",
       "998 -0.009306 -0.009867 -0.010143  ... -0.007472 -0.005698 -0.003206   \n",
       "999  0.008174  0.008176  0.007942  ...  0.006020  0.006687  0.007170   \n",
       "\n",
       "          250       251       252       253       254       255       256  \n",
       "1    0.001587  0.001841  0.000575  0.001187  0.002046  0.001886  0.002628  \n",
       "2    0.005547  0.006535  0.007792  0.008272  0.008922  0.009115  0.008782  \n",
       "3    0.003708  0.000989 -0.001881 -0.004819 -0.007318 -0.009326 -0.010401  \n",
       "4    0.007210  0.007103  0.007050  0.007189  0.007495  0.007642  0.007892  \n",
       "5   -0.009310 -0.009821 -0.009019 -0.007089 -0.005824 -0.003152 -0.002219  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995  0.007043  0.007077  0.007282  0.007349  0.007484  0.007647  0.008022  \n",
       "996 -0.005976 -0.005619 -0.005147 -0.004462 -0.003799 -0.003172 -0.002490  \n",
       "997  0.003591  0.002326  0.001391  0.000361 -0.000675 -0.001008 -0.000936  \n",
       "998 -0.000475  0.002331  0.004923  0.007142  0.008841  0.009958  0.010485  \n",
       "999  0.007336  0.007188  0.006765  0.006229  0.007247  0.007944  0.008060  \n",
       "\n",
       "[999 rows x 257 columns]>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SET_SIZE = 1000\n",
    "\n",
    "data = pd.read_csv(\"./data/dataset_train_2024.csv\", header=None)\n",
    "data = data.iloc[1:TRAIN_SET_SIZE, : ]\n",
    "\n",
    "trainData = data.iloc[:, 258]\n",
    "trainLabels = data.iloc[:, : 257]\n",
    "\n",
    "trainData.head\n",
    "trainLabels.head\n",
    "\n",
    "\n",
    "\n",
    "# def custom_collater(batch):\n",
    "#     \"\"\"\n",
    "#     Custom collater function for batching sequence data.\n",
    "\n",
    "#     Args:\n",
    "#         batch (list of tuples): Each tuple contains (data, label).\n",
    "        \n",
    "#     Returns:\n",
    "#         dict: Batched input data with padding mask.\n",
    "#         dict: Batched target labels with padding mask.\n",
    "#     \"\"\"\n",
    "#     # Extract sequences and labels from the batch\n",
    "#     sequences, labels = zip(*batch)\n",
    "    \n",
    "#     # Convert to PyTorch tensors\n",
    "#     sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "#     labels = [torch.tensor(lbl, dtype=torch.long) for lbl in labels]\n",
    "    \n",
    "#     # Determine max sequence length for padding\n",
    "#     max_seq_len = max([len(seq) for seq in sequences])\n",
    "#     max_label_len = max([len(lbl) for lbl in labels])\n",
    "    \n",
    "#     # Pad sequences and labels\n",
    "#     padded_sequences = torch.zeros(len(sequences), max_seq_len, dtype=torch.long)\n",
    "#     sequence_padding_mask = torch.ones(len(sequences), max_seq_len, dtype=torch.bool)\n",
    "    \n",
    "#     for i, seq in enumerate(sequences):\n",
    "#         padded_sequences[i, :len(seq)] = seq\n",
    "#         sequence_padding_mask[i, :len(seq)] = False\n",
    "    \n",
    "#     padded_labels = torch.zeros(len(labels), max_label_len, dtype=torch.long)\n",
    "#     label_padding_mask = torch.ones(len(labels), max_label_len, dtype=torch.bool)\n",
    "    \n",
    "#     for i, lbl in enumerate(labels):\n",
    "#         padded_labels[i, :len(lbl)] = lbl\n",
    "#         label_padding_mask[i, :len(lbl)] = False\n",
    "\n",
    "#     # Package into dictionaries\n",
    "#     src = {\n",
    "#         \"ids\": padded_sequences,\n",
    "#         \"padding_mask\": sequence_padding_mask,\n",
    "#     }\n",
    "#     tgt = {\n",
    "#         \"ids\": padded_labels,\n",
    "#         \"padding_mask\": label_padding_mask,\n",
    "#     }\n",
    "\n",
    "#     return src, tgt\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000019306FF3C90>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m loss_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(numbers_loader_train)\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnumbers_loader_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mique\\Documents\\UNI\\MATT\\MLEARN\\Competition\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\mique\\Documents\\UNI\\MATT\\MLEARN\\Competition\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\mique\\Documents\\UNI\\MATT\\MLEARN\\Competition\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 34\u001b[0m, in \u001b[0;36mcustom_collater\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Convert to PyTorch tensors\u001b[39;00m\n\u001b[0;32m     33\u001b[0m sequences \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(seq, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences]\n\u001b[1;32m---> 34\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlbl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlbl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Determine max sequence length for padding\u001b[39;00m\n\u001b[0;32m     37\u001b[0m max_seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences])\n",
      "Cell \u001b[1;32mIn[28], line 34\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Convert to PyTorch tensors\u001b[39;00m\n\u001b[0;32m     33\u001b[0m sequences \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(seq, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences]\n\u001b[1;32m---> 34\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlbl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m lbl \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Determine max sequence length for padding\u001b[39;00m\n\u001b[0;32m     37\u001b[0m max_seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences])\n",
      "\u001b[1;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "lr = 5e-4\n",
    "batch_size = 32\n",
    "log_interval = 50\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "numbers_loader_train = DataLoader(\n",
    "    list(zip(trainData.values, trainLabels.values)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collater,\n",
    ")\n",
    "\n",
    "transformer_encoder_cfg = {\n",
    "    \"num_layers\": 3,\n",
    "    \"embed_dim\": 256,\n",
    "    \"ffn_dim\": 1024,\n",
    "    \"num_heads\": 4,\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "transformer_decoder_cfg = {\n",
    "    \"num_layers\": 3,\n",
    "    \"embed_dim\": 256,\n",
    "    \"ffn_dim\": 1024,\n",
    "    \"num_heads\": 4,\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "model = Transformer(transformer_encoder_cfg, transformer_decoder_cfg)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = F.nll_loss\n",
    "\n",
    "print(\"Training model...\")\n",
    "\n",
    "loss_avg = 0\n",
    "print(numbers_loader_train)\n",
    "for i, (src, tgt) in enumerate(numbers_loader_train):\n",
    "    print(src)\n",
    "    src = {k: v.to(device) for k, v in src.items()}\n",
    "    print(src)\n",
    "    tgt = {k: v.to(device) for k, v in tgt.items()}\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(\n",
    "        src['ids'],\n",
    "        tgt['ids'][:-1],\n",
    "        src['padding_mask'],\n",
    "        tgt['padding_mask'][:, :-1],\n",
    "    )\n",
    "\n",
    "    loss = criterion(\n",
    "        output.reshape(-1, output.size(-1)),\n",
    "        tgt['ids'][1:].flatten()\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_avg += loss.item()\n",
    "    if (i+1) % log_interval == 0:\n",
    "        loss_avg /= log_interval\n",
    "        print(f\"{i+1}/{len(numbers_loader_train)}\\tLoss: {loss_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mique\\Documents\\UNI\\MATT\\MLEARN\\Competition\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/10, Loss: 1.6369\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 105\u001b[0m\n\u001b[0;32m    103\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m    104\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 105\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mique\\Documents\\UNI\\MATT\\MLEARN\\Competition\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mique\\Documents\\UNI\\MATT\\MLEARN\\Competition\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\mique\\Documents\\UNI\\MATT\\MLEARN\\Competition\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\mique\\Documents\\UNI\\MATT\\MLEARN\\Competition\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mique\\Documents\\UNI\\MATT\\MLEARN\\Competition\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mique\\Documents\\UNI\\MATT\\MLEARN\\Competition\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 432\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        # Load data from CSV\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Extract features\n",
    "        self.sequences_1 = data.iloc[:, 1:129].values  # Columns 1-128 (1-based indexing)\n",
    "        self.sequences_2 = data.iloc[:, 129:257].values  # Columns 129-256\n",
    "        self.extra_feature = data.iloc[:, 257].values  # Column 257\n",
    "        self.features = torch.tensor(\n",
    "            np.hstack([self.sequences_1, self.sequences_2, self.extra_feature.reshape(-1, 1)]),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        # Encode labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = torch.tensor(self.label_encoder.fit_transform(data.iloc[:, -1]), dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, ff_dim, num_classes, dropout=0.1):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, input_dim, embed_dim))  # Match input_dim\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, feature_dim = x.size()\n",
    "        if feature_dim != self.positional_encoding.size(1):\n",
    "            raise ValueError(f\"Feature dimension mismatch: Expected {self.positional_encoding.size(1)}, got {feature_dim}\")\n",
    "\n",
    "        x = self.embedding(x).unsqueeze(1)  # (batch_size, 1, embed_dim)\n",
    "        x += self.positional_encoding[:, :1, :]  # Add positional encoding\n",
    "        x = x.permute(1, 0, 2)  # (seq_len=1, batch_size, embed_dim)\n",
    "        x = self.transformer_encoder(x)  # (seq_len=1, batch_size, embed_dim)\n",
    "        x = x.mean(dim=0)  # (batch_size, embed_dim)\n",
    "        return self.fc(x)  # (batch_size, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "csv_path = \"data/dataset_train_2024.csv\"  # Path to the dataset CSV file\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "learning_rate = 1e-4\n",
    "input_dim = 257  # 128+128+1\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 30\n",
    "ff_dim = 512\n",
    "num_classes = 5  # Adjust based on the dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = CustomDataset(csv_path)\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size  # 20% for testing\n",
    "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = TransformerClassifier(input_dim, embed_dim, num_heads, num_layers, ff_dim, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "print(\"Training the model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Testing Loop\n",
    "print(\"Testing the model...\")\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        outputs = model(features)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
