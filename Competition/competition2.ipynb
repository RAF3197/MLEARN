{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the multiHead attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    \"\"\" Computes the Scaled Dot-Product Attention\n",
    "\n",
    "    Args:\n",
    "        q (torch.FloatTensor):  Query Tensor   (... x T_q x d_q)\n",
    "        k (torch.FloatTensor):  Key Tensor     (... x T_k x d_k)\n",
    "        v (torch.FloatTensor):  Value Tensor   (... x T_v x d_v)\n",
    "        mask (torch.BoolTensor): Attention mask (... x T_q x T_k)\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: Result of the SDPA  (... x T_q x d_v)\n",
    "        torch.FloatTensor: Attention map       (... x T_q x T_k)\n",
    "\n",
    "    \"\"\"\n",
    "    assert q.size(-1) == k.size(-1), \"Query and Key dimensions must coincide\"\n",
    "\n",
    "    # TODO: Matrix multiplication of the queries and the keys (use torch.matmul)\n",
    "    #attn_logits =\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "\n",
    "    # TODO: Scale attn_logits (see the SDPA formula, d_k is the last dim of k)\n",
    "    #attn_logits = \n",
    "    attn_logits = attn_logits/torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))\n",
    "\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask, -float(\"inf\"))\n",
    "\n",
    "    # TODO: Compute the attention weights (see the SDPA formula, use dim=-1)\n",
    "    #attention =\n",
    "    attention = torch.softmax(attn_logits, dim=-1)\n",
    "\n",
    "    output = torch.matmul(attention, v)\n",
    "\n",
    "    return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \\\n",
    "            \"Embedding dimension must be multiple of the number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.proj_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization\n",
    "        nn.init.xavier_uniform_(self.proj_q.weight)\n",
    "        nn.init.xavier_uniform_(self.proj_k.weight)\n",
    "        nn.init.xavier_uniform_(self.proj_v.weight)\n",
    "        nn.init.xavier_uniform_(self.proj_o.weight)\n",
    "        self.proj_q.bias.data.fill_(0)\n",
    "        self.proj_k.bias.data.fill_(0)\n",
    "        self.proj_v.bias.data.fill_(0)\n",
    "        self.proj_o.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(1)\n",
    "\n",
    "        q = self.proj_q(q)\n",
    "        k = self.proj_k(k)\n",
    "        v = self.proj_v(v)\n",
    "\n",
    "        # TODO: Split the tensors into multiple heads\n",
    "        #  T x B x embed_dim -> T x B x num_heads x head_dim\n",
    "        q = q.reshape(q.size(0), batch_size, self.num_heads, self.head_dim)\n",
    "        k = k.reshape(k.size(0), batch_size, self.num_heads, self.head_dim)\n",
    "        v = v.reshape(v.size(0), batch_size, self.num_heads, self.head_dim)\n",
    "\n",
    "        # The last two dimensions must be sequence length and the head dimension,\n",
    "        # to make it work with the scaled dot-product function.\n",
    "        # TODO: Rearrange the dimensions\n",
    "        # T x B x num_heads x head_dim -> B x num_heads x T x head_dim\n",
    "        q = q.permute(1, 2, 0, 3)\n",
    "        k = k.permute(1, 2, 0, 3)\n",
    "        v = v.permute(1, 2, 0, 3)\n",
    "\n",
    "        # Apply the same mask to all the heads\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # TODO: Call the scaled dot-product function (remember to pass the mask!)\n",
    "        output_heads, attn_w = scaled_dot_product(q, k, v, mask)\n",
    "\n",
    "        # B x num_heads x T x head_dim -> T x B x num_heads x head_dim\n",
    "        output_heads = output_heads.permute(2, 0, 1, 3)\n",
    "\n",
    "        # T x B x num_heads x head_dim -> T x B x embed_dim\n",
    "        output_cat = output_heads.reshape(-1, batch_size, self.embed_dim)\n",
    "        output = self.proj_o(output_cat)\n",
    "\n",
    "        return output, attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimensionality\n",
    "            max_len (int): Maximum length of a sequence to expect\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create matrix of (T x embed_dim) representing the positional encoding\n",
    "        # for max_len inputs\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimensionality (input, output & self-attention)\n",
    "            ffn_dim (int): Inner dimensionality in the FFN\n",
    "            num_heads (int): Number of heads of the multi-head attention block\n",
    "            dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiheadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(ffn_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None, return_att=False):\n",
    "        src_len, batch_size, _ = x.shape\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(x.shape[1], x.shape[0]).bool().to(x.device)\n",
    "\n",
    "        selfattn_mask = mask.unsqueeze(-2)\n",
    "\n",
    "        # TODO: Self-Attention block\n",
    "        selfattn_out, selfattn_w = self.self_attn(x, x, x, selfattn_mask)\n",
    "        selfattn_out = self.dropout(selfattn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (1)\n",
    "        x = self.norm1(x + selfattn_out)\n",
    "\n",
    "        # TODO: FFN block\n",
    "        ffn_out = self.ffn(x)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (2)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "\n",
    "        if return_att:\n",
    "            return x, selfattn_w\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        # Create an embedding table (T x B -> T x B x embed_dim)\n",
    "        # self.embedding = nn.Embedding(, embed_dim)\n",
    "\n",
    "        # Create the positional encoding with the class defined before\n",
    "        self.pos_enc = PositionalEncoding(embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, ffn_dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None, return_att=False):\n",
    "        #x = self.embedding(x)\n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        selfattn_ws = []\n",
    "        for l in self.layers:\n",
    "            if return_att:\n",
    "                x, selfattn_w = l(x, mask=mask, return_att=True)\n",
    "                selfattn_ws.append(selfattn_w)\n",
    "            else:\n",
    "                x = l(x, mask=mask, return_att=False)\n",
    "\n",
    "        if return_att:\n",
    "            selfattn_ws = torch.stack(selfattn_ws, dim=1)\n",
    "            return x, selfattn_ws\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimensionality (input, output & self-attention)\n",
    "            ffn_dim (int): Inner dimensionality in the FFN\n",
    "            num_heads (int): Number of heads of the multi-head attention block\n",
    "            dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiheadAttention(embed_dim, num_heads)\n",
    "        self.encdec_attn = MultiheadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(ffn_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, memory, mask=None, memory_mask=None, return_att=False):\n",
    "        tgt_len, batch_size, _ = x.shape\n",
    "        src_len, _, _ = memory.shape\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(x.shape[1], x.shape[0])\n",
    "            mask = mask.bool().to(x.device)\n",
    "        if memory_mask is None:\n",
    "            memory_mask = torch.zeros(memory.shape[1], memory.shape[0])\n",
    "            memory_mask = memory_mask.bool().to(memory.device)\n",
    "\n",
    "\n",
    "        subsequent_mask = torch.triu(torch.ones(batch_size, tgt_len, tgt_len), 1)\n",
    "        subsequent_mask = subsequent_mask.bool().to(mask.device)\n",
    "        selfattn_mask = subsequent_mask + mask.unsqueeze(-2)\n",
    "\n",
    "        attn_mask = memory_mask.unsqueeze(-2)\n",
    "\n",
    "        # TODO: Self-Attention block\n",
    "        selfattn_out, selfattn_w = self.self_attn(x, x, x, selfattn_mask)\n",
    "        selfattn_out = self.dropout(selfattn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (1)\n",
    "        x = self.norm1(x + selfattn_out)\n",
    "\n",
    "        # TODO: Encoder-Decoder Attention block\n",
    "        attn_out, attn_w = self.encdec_attn(x, memory, memory, attn_mask)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (2)\n",
    "        x = self.norm2(x + attn_out)\n",
    "\n",
    "        # TODO: FFN block\n",
    "        ffn_out = self.ffn(x)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (3)\n",
    "        x = self.norm3(x + ffn_out)\n",
    "\n",
    "        if return_att:\n",
    "            return x, selfattn_w, attn_w\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        # Create an embedding table (T x B -> T x B x embed_dim)\n",
    "        # self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # Create the positional encoding with the class defined before\n",
    "        self.pos_enc = PositionalEncoding(embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(embed_dim, ffn_dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Add a projection layer (T x B x embed_dim -> T x B x vocab_size)\n",
    "        # self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, memory, mask=None, memory_mask=None, return_att=False):\n",
    "        #x = self.embedding(x)\n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        selfattn_ws = []\n",
    "        attn_ws = []\n",
    "        for l in self.layers:\n",
    "            if return_att:\n",
    "                x, selfattn_w, attn_w = l(\n",
    "                    x, memory, mask=mask, memory_mask=memory_mask, return_att=True\n",
    "                )\n",
    "                selfattn_ws.append(selfattn_w)\n",
    "                attn_ws.append(attn_w)\n",
    "            else:\n",
    "                x = l(\n",
    "                    x, memory, mask=mask, memory_mask=memory_mask, return_att=False\n",
    "                )\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "\n",
    "        if return_att:\n",
    "            selfattn_ws = torch.stack(selfattn_ws, dim=1)\n",
    "            attn_ws = torch.stack(attn_ws, dim=1)\n",
    "            return x, selfattn_ws, attn_ws\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder_config, decoder_config):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = TransformerEncoder(**encoder_config)\n",
    "        #self.decoder = TransformerDecoder(**decoder_config)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\" Forward method\n",
    "\n",
    "        Method used at training time, when the target is known. The target tensor\n",
    "        passed to the decoder is shifted to the right (starting with BOS\n",
    "        symbol). Then, the output of the decoder starts directly with the first\n",
    "        token of the sentence.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Compute the encoder output\n",
    "        encoder_out = self.encoder(src, src_mask)\n",
    "\n",
    "        # TODO: Compute the decoder output\n",
    "        #decoder_out = self.decoder(\n",
    "        #    x=tgt,\n",
    "        #   memory=encoder_out,\n",
    "        #    mask=tgt_mask,\n",
    "        #    memory_mask=src_mask\n",
    "        #)\n",
    "\n",
    "        #return decoder_out\n",
    "        return encoder_out\n",
    "\n",
    "    def generate(self, src, src_mask=None, bos_idx=0, max_len=50):\n",
    "        \"\"\" Generate method\n",
    "\n",
    "        Method used at inference time, when the target is unknown. It\n",
    "        iteratively passes to the decoder the sequence generated so far\n",
    "        and appends the new token to the input again. It uses a Greedy\n",
    "        decoding (argmax).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Compute the encoder output\n",
    "        encoder_out = self.encoder(src, src_mask)\n",
    "\n",
    "        output = torch.LongTensor([bos_idx])\\\n",
    "                    .expand(1, encoder_out.size(1)).to(src.device)\n",
    "        for i in range(max_len):\n",
    "            # TODO: Get the new token\n",
    "            new_token = self.decoder(\n",
    "                x=output,\n",
    "                memory=encoder_out,\n",
    "                memory_mask=src_mask\n",
    "            )[-1].argmax(-1)\n",
    "\n",
    "            output = torch.cat([output, new_token.unsqueeze(0)], dim=0)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SET_SIZE = 1000\n",
    "\n",
    "data = pd.read_csv(\"./data/dataset_train_2024.csv\", header=None)\n",
    "data = data.iloc[1:TRAIN_SET_SIZE, : ]\n",
    "\n",
    "trainData = data.iloc[:, 258]\n",
    "trainLabels = data.iloc[:, : 257]\n",
    "\n",
    "trainData.head\n",
    "trainLabels.head\n",
    "\n",
    "\n",
    "\n",
    "# def custom_collater(batch):\n",
    "#     \"\"\"\n",
    "#     Custom collater function for batching sequence data.\n",
    "\n",
    "#     Args:\n",
    "#         batch (list of tuples): Each tuple contains (data, label).\n",
    "        \n",
    "#     Returns:\n",
    "#         dict: Batched input data with padding mask.\n",
    "#         dict: Batched target labels with padding mask.\n",
    "#     \"\"\"\n",
    "#     # Extract sequences and labels from the batch\n",
    "#     sequences, labels = zip(*batch)\n",
    "    \n",
    "#     # Convert to PyTorch tensors\n",
    "#     sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "#     labels = [torch.tensor(lbl, dtype=torch.long) for lbl in labels]\n",
    "    \n",
    "#     # Determine max sequence length for padding\n",
    "#     max_seq_len = max([len(seq) for seq in sequences])\n",
    "#     max_label_len = max([len(lbl) for lbl in labels])\n",
    "    \n",
    "#     # Pad sequences and labels\n",
    "#     padded_sequences = torch.zeros(len(sequences), max_seq_len, dtype=torch.long)\n",
    "#     sequence_padding_mask = torch.ones(len(sequences), max_seq_len, dtype=torch.bool)\n",
    "    \n",
    "#     for i, seq in enumerate(sequences):\n",
    "#         padded_sequences[i, :len(seq)] = seq\n",
    "#         sequence_padding_mask[i, :len(seq)] = False\n",
    "    \n",
    "#     padded_labels = torch.zeros(len(labels), max_label_len, dtype=torch.long)\n",
    "#     label_padding_mask = torch.ones(len(labels), max_label_len, dtype=torch.bool)\n",
    "    \n",
    "#     for i, lbl in enumerate(labels):\n",
    "#         padded_labels[i, :len(lbl)] = lbl\n",
    "#         label_padding_mask[i, :len(lbl)] = False\n",
    "\n",
    "#     # Package into dictionaries\n",
    "#     src = {\n",
    "#         \"ids\": padded_sequences,\n",
    "#         \"padding_mask\": sequence_padding_mask,\n",
    "#     }\n",
    "#     tgt = {\n",
    "#         \"ids\": padded_labels,\n",
    "#         \"padding_mask\": label_padding_mask,\n",
    "#     }\n",
    "\n",
    "#     return src, tgt\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "batch_size = 32\n",
    "log_interval = 50\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "numbers_loader_train = DataLoader(\n",
    "    list(zip(trainData.values, trainLabels.values)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collater,\n",
    ")\n",
    "\n",
    "transformer_encoder_cfg = {\n",
    "    \"num_layers\": 3,\n",
    "    \"embed_dim\": 256,\n",
    "    \"ffn_dim\": 1024,\n",
    "    \"num_heads\": 4,\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "transformer_decoder_cfg = {\n",
    "    \"num_layers\": 3,\n",
    "    \"embed_dim\": 256,\n",
    "    \"ffn_dim\": 1024,\n",
    "    \"num_heads\": 4,\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "model = Transformer(transformer_encoder_cfg, transformer_decoder_cfg)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = F.nll_loss\n",
    "\n",
    "print(\"Training model...\")\n",
    "\n",
    "loss_avg = 0\n",
    "print(numbers_loader_train)\n",
    "for i, (src, tgt) in enumerate(numbers_loader_train):\n",
    "    print(src)\n",
    "    src = {k: v.to(device) for k, v in src.items()}\n",
    "    print(src)\n",
    "    tgt = {k: v.to(device) for k, v in tgt.items()}\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(\n",
    "        src['ids'],\n",
    "        tgt['ids'][:-1],\n",
    "        src['padding_mask'],\n",
    "        tgt['padding_mask'][:, :-1],\n",
    "    )\n",
    "\n",
    "    loss = criterion(\n",
    "        output.reshape(-1, output.size(-1)),\n",
    "        tgt['ids'][1:].flatten()\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_avg += loss.item()\n",
    "    if (i+1) % log_interval == 0:\n",
    "        loss_avg /= log_interval\n",
    "        print(f\"{i+1}/{len(numbers_loader_train)}\\tLoss: {loss_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        # Load data from CSV\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Extract features\n",
    "        self.sequences_1 = data.iloc[:, 1:129].values  # Columns 1-128 (1-based indexing)\n",
    "        self.sequences_2 = data.iloc[:, 129:257].values  # Columns 129-256\n",
    "        #self.extra_feature = data.iloc[:, 257].values  # Column 257\n",
    "        self.features = torch.tensor(\n",
    "            #np.hstack([self.sequences_1, self.sequences_2, self.extra_feature.reshape(-1, 1)]),ç\n",
    "            np.hstack([self.sequences_1, self.sequences_2]),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        # Encode labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = torch.tensor(self.label_encoder.fit_transform(data.iloc[:, -1]), dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "    \n",
    "    def inverseTransform(self, array):\n",
    "        return self.label_encoder.inverse_transform(array)\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, ff_dim, num_classes, dropout=0.3):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, input_dim, embed_dim))  # Match input_dim\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, feature_dim = x.size()\n",
    "        if feature_dim != self.positional_encoding.size(1):\n",
    "            raise ValueError(f\"Feature dimension mismatch: Expected {self.positional_encoding.size(1)}, got {feature_dim}\")\n",
    "\n",
    "        x = self.embedding(x).unsqueeze(1)  # (batch_size, 1, embed_dim)\n",
    "        x += self.positional_encoding[:, :1, :]  # Add positional encoding\n",
    "        x = x.permute(1, 0, 2)  # (seq_len=1, batch_size, embed_dim)\n",
    "        x = self.transformer_encoder(x)  # (seq_len=1, batch_size, embed_dim)\n",
    "        x = x.mean(dim=0)  # (batch_size, embed_dim)\n",
    "        return self.fc(x)  # (batch_size, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "csv_path = \"data/dataset_train_2024.csv\"  # Path to the dataset CSV file\n",
    "batch_size = 32\n",
    "epochs = 1500\n",
    "learning_rate = 5e-4\n",
    "input_dim = 256  # 128+128+1\n",
    "embed_dim = 256\n",
    "num_heads = 4\n",
    "num_layers = 1\n",
    "ff_dim = 4 * embed_dim\n",
    "num_classes = 5  # Adjust based on the dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = CustomDataset(csv_path)\n",
    "train_size = int(0.9 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size  # 20% for testing\n",
    "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = TransformerClassifier(input_dim, embed_dim, num_heads, num_layers, ff_dim, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=epochs\n",
    ")\n",
    "\n",
    "#from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "#scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/1500, Loss: 1.6323\n",
      "Epoch 2/1500, Loss: 1.6251\n",
      "Epoch 3/1500, Loss: 1.6229\n",
      "Epoch 4/1500, Loss: 1.6203\n",
      "Epoch 5/1500, Loss: 1.6184\n",
      "Epoch 6/1500, Loss: 1.6184\n",
      "Epoch 7/1500, Loss: 1.6153\n",
      "Epoch 8/1500, Loss: 1.6145\n",
      "Epoch 9/1500, Loss: 1.6129\n",
      "Epoch 10/1500, Loss: 1.6146\n",
      "Epoch 11/1500, Loss: 1.6119\n",
      "Epoch 12/1500, Loss: 1.6129\n",
      "Epoch 13/1500, Loss: 1.6122\n",
      "Epoch 14/1500, Loss: 1.6099\n",
      "Epoch 15/1500, Loss: 1.6098\n",
      "Epoch 16/1500, Loss: 1.6061\n",
      "Epoch 17/1500, Loss: 1.6077\n",
      "Epoch 18/1500, Loss: 1.6045\n",
      "Epoch 19/1500, Loss: 1.5985\n",
      "Epoch 20/1500, Loss: 1.5829\n",
      "Epoch 21/1500, Loss: 1.5546\n",
      "Epoch 22/1500, Loss: 1.5114\n",
      "Epoch 23/1500, Loss: 1.4867\n",
      "Epoch 24/1500, Loss: 1.4734\n",
      "Epoch 25/1500, Loss: 1.4502\n",
      "Epoch 26/1500, Loss: 1.4207\n",
      "Epoch 27/1500, Loss: 1.4002\n",
      "Epoch 28/1500, Loss: 1.3853\n",
      "Epoch 29/1500, Loss: 1.3788\n",
      "Epoch 30/1500, Loss: 1.3693\n",
      "Epoch 31/1500, Loss: 1.3603\n",
      "Epoch 32/1500, Loss: 1.3503\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "print(\"Training the model...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n",
      "F1 Score: 0.6012\n",
      "Test Accuracy: 0.6125\n"
     ]
    }
   ],
   "source": [
    "# Testing Loop\n",
    "print(\"Testing the model...\")\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        outputs = model(features)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')  # or 'macro', 'micro', depending on your use case\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Feature dimension mismatch: Expected 256, got 257",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Send inputs to the same device as the model\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Get predicted class\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Save predictions and indices\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 58\u001b[0m, in \u001b[0;36mTransformerClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m batch_size, feature_dim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_dim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature dimension mismatch: Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, 1, embed_dim)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding[:, :\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Add positional encoding\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Feature dimension mismatch: Expected 256, got 257"
     ]
    }
   ],
   "source": [
    "#Using the model for prediction with the evaluation dataset\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define the dataset class\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.data.iloc[idx].values.astype('float32')  # Adjust for your data type\n",
    "        if self.transform:\n",
    "            inputs = self.transform(inputs)\n",
    "        return inputs\n",
    "\n",
    "# Load the unlabeled dataset\n",
    "csv_path = \"data/dataset_test_no_label_2024.csv\"  # Path to the dataset CSV file\n",
    "unlabeled_df = pd.read_csv(csv_path)  # Update the filename\n",
    "unlabeled_df = unlabeled_df.drop(unlabeled_df.columns[0], axis=1)\n",
    "unlabeled_dataset = UnlabeledDataset(unlabeled_df)\n",
    "unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Store predictions and indices\n",
    "predictions = []\n",
    "indices = []\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    for idx, inputs in enumerate(unlabeled_dataloader):\n",
    "        inputs = inputs.to(device)  # Send inputs to the same device as the model\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)  # Get predicted class\n",
    "        \n",
    "        # Save predictions and indices\n",
    "        start_idx = idx * unlabeled_dataloader.batch_size\n",
    "        batch_indices = list(range(start_idx, start_idx + len(inputs)))  # Adjusting the index properly\n",
    "        indices.extend(batch_indices)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Create a DataFrame with indices and predictions\n",
    "output_df = pd.DataFrame({\"ID\": indices, \"MODULATION\": dataset.inverseTransform(predictions)})\n",
    "\n",
    "# Save to a CSV file\n",
    "output_df.to_csv(\"predictions_with_indices.csv\", index=False)\n",
    "\n",
    "print(\"Predictions saved to 'predictions_with_indices.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
