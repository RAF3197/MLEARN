{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the multiHead attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    \"\"\" Computes the Scaled Dot-Product Attention\n",
    "\n",
    "    Args:\n",
    "        q (torch.FloatTensor):  Query Tensor   (... x T_q x d_q)\n",
    "        k (torch.FloatTensor):  Key Tensor     (... x T_k x d_k)\n",
    "        v (torch.FloatTensor):  Value Tensor   (... x T_v x d_v)\n",
    "        mask (torch.BoolTensor): Attention mask (... x T_q x T_k)\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: Result of the SDPA  (... x T_q x d_v)\n",
    "        torch.FloatTensor: Attention map       (... x T_q x T_k)\n",
    "\n",
    "    \"\"\"\n",
    "    assert q.size(-1) == k.size(-1), \"Query and Key dimensions must coincide\"\n",
    "\n",
    "    # TODO: Matrix multiplication of the queries and the keys (use torch.matmul)\n",
    "    #attn_logits =\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "\n",
    "    # TODO: Scale attn_logits (see the SDPA formula, d_k is the last dim of k)\n",
    "    #attn_logits = \n",
    "    attn_logits = attn_logits/torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))\n",
    "\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask, -float(\"inf\"))\n",
    "\n",
    "    # TODO: Compute the attention weights (see the SDPA formula, use dim=-1)\n",
    "    #attention =\n",
    "    attention = torch.softmax(attn_logits, dim=-1)\n",
    "\n",
    "    output = torch.matmul(attention, v)\n",
    "\n",
    "    return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \\\n",
    "            \"Embedding dimension must be multiple of the number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.proj_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization\n",
    "        nn.init.xavier_uniform_(self.proj_q.weight)\n",
    "        nn.init.xavier_uniform_(self.proj_k.weight)\n",
    "        nn.init.xavier_uniform_(self.proj_v.weight)\n",
    "        nn.init.xavier_uniform_(self.proj_o.weight)\n",
    "        self.proj_q.bias.data.fill_(0)\n",
    "        self.proj_k.bias.data.fill_(0)\n",
    "        self.proj_v.bias.data.fill_(0)\n",
    "        self.proj_o.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(1)\n",
    "\n",
    "        q = self.proj_q(q)\n",
    "        k = self.proj_k(k)\n",
    "        v = self.proj_v(v)\n",
    "\n",
    "        # TODO: Split the tensors into multiple heads\n",
    "        #  T x B x embed_dim -> T x B x num_heads x head_dim\n",
    "        q = q.reshape(q.size(0), batch_size, self.num_heads, self.head_dim)\n",
    "        k = k.reshape(k.size(0), batch_size, self.num_heads, self.head_dim)\n",
    "        v = v.reshape(v.size(0), batch_size, self.num_heads, self.head_dim)\n",
    "\n",
    "        # The last two dimensions must be sequence length and the head dimension,\n",
    "        # to make it work with the scaled dot-product function.\n",
    "        # TODO: Rearrange the dimensions\n",
    "        # T x B x num_heads x head_dim -> B x num_heads x T x head_dim\n",
    "        q = q.permute(1, 2, 0, 3)\n",
    "        k = k.permute(1, 2, 0, 3)\n",
    "        v = v.permute(1, 2, 0, 3)\n",
    "\n",
    "        # Apply the same mask to all the heads\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # TODO: Call the scaled dot-product function (remember to pass the mask!)\n",
    "        output_heads, attn_w = scaled_dot_product(q, k, v, mask)\n",
    "\n",
    "        # B x num_heads x T x head_dim -> T x B x num_heads x head_dim\n",
    "        output_heads = output_heads.permute(2, 0, 1, 3)\n",
    "\n",
    "        # T x B x num_heads x head_dim -> T x B x embed_dim\n",
    "        output_cat = output_heads.reshape(-1, batch_size, self.embed_dim)\n",
    "        output = self.proj_o(output_cat)\n",
    "\n",
    "        return output, attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimensionality\n",
    "            max_len (int): Maximum length of a sequence to expect\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create matrix of (T x embed_dim) representing the positional encoding\n",
    "        # for max_len inputs\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimensionality (input, output & self-attention)\n",
    "            ffn_dim (int): Inner dimensionality in the FFN\n",
    "            num_heads (int): Number of heads of the multi-head attention block\n",
    "            dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiheadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(ffn_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None, return_att=False):\n",
    "        src_len, batch_size, _ = x.shape\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(x.shape[1], x.shape[0]).bool().to(x.device)\n",
    "\n",
    "        selfattn_mask = mask.unsqueeze(-2)\n",
    "\n",
    "        # TODO: Self-Attention block\n",
    "        selfattn_out, selfattn_w = self.self_attn(x, x, x, selfattn_mask)\n",
    "        selfattn_out = self.dropout(selfattn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (1)\n",
    "        x = self.norm1(x + selfattn_out)\n",
    "\n",
    "        # TODO: FFN block\n",
    "        ffn_out = self.ffn(x)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (2)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "\n",
    "        if return_att:\n",
    "            return x, selfattn_w\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        # Create an embedding table (T x B -> T x B x embed_dim)\n",
    "        # self.embedding = nn.Embedding(, embed_dim)\n",
    "\n",
    "        # Create the positional encoding with the class defined before\n",
    "        self.pos_enc = PositionalEncoding(embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, ffn_dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None, return_att=False):\n",
    "        #x = self.embedding(x)\n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        selfattn_ws = []\n",
    "        for l in self.layers:\n",
    "            if return_att:\n",
    "                x, selfattn_w = l(x, mask=mask, return_att=True)\n",
    "                selfattn_ws.append(selfattn_w)\n",
    "            else:\n",
    "                x = l(x, mask=mask, return_att=False)\n",
    "\n",
    "        if return_att:\n",
    "            selfattn_ws = torch.stack(selfattn_ws, dim=1)\n",
    "            return x, selfattn_ws\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Embedding dimensionality (input, output & self-attention)\n",
    "            ffn_dim (int): Inner dimensionality in the FFN\n",
    "            num_heads (int): Number of heads of the multi-head attention block\n",
    "            dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiheadAttention(embed_dim, num_heads)\n",
    "        self.encdec_attn = MultiheadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(ffn_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, memory, mask=None, memory_mask=None, return_att=False):\n",
    "        tgt_len, batch_size, _ = x.shape\n",
    "        src_len, _, _ = memory.shape\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(x.shape[1], x.shape[0])\n",
    "            mask = mask.bool().to(x.device)\n",
    "        if memory_mask is None:\n",
    "            memory_mask = torch.zeros(memory.shape[1], memory.shape[0])\n",
    "            memory_mask = memory_mask.bool().to(memory.device)\n",
    "\n",
    "\n",
    "        subsequent_mask = torch.triu(torch.ones(batch_size, tgt_len, tgt_len), 1)\n",
    "        subsequent_mask = subsequent_mask.bool().to(mask.device)\n",
    "        selfattn_mask = subsequent_mask + mask.unsqueeze(-2)\n",
    "\n",
    "        attn_mask = memory_mask.unsqueeze(-2)\n",
    "\n",
    "        # TODO: Self-Attention block\n",
    "        selfattn_out, selfattn_w = self.self_attn(x, x, x, selfattn_mask)\n",
    "        selfattn_out = self.dropout(selfattn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (1)\n",
    "        x = self.norm1(x + selfattn_out)\n",
    "\n",
    "        # TODO: Encoder-Decoder Attention block\n",
    "        attn_out, attn_w = self.encdec_attn(x, memory, memory, attn_mask)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (2)\n",
    "        x = self.norm2(x + attn_out)\n",
    "\n",
    "        # TODO: FFN block\n",
    "        ffn_out = self.ffn(x)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "\n",
    "        # TODO: Add + normalize block (3)\n",
    "        x = self.norm3(x + ffn_out)\n",
    "\n",
    "        if return_att:\n",
    "            return x, selfattn_w, attn_w\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        # Create an embedding table (T x B -> T x B x embed_dim)\n",
    "        # self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # Create the positional encoding with the class defined before\n",
    "        self.pos_enc = PositionalEncoding(embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(embed_dim, ffn_dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Add a projection layer (T x B x embed_dim -> T x B x vocab_size)\n",
    "        # self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, memory, mask=None, memory_mask=None, return_att=False):\n",
    "        #x = self.embedding(x)\n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        selfattn_ws = []\n",
    "        attn_ws = []\n",
    "        for l in self.layers:\n",
    "            if return_att:\n",
    "                x, selfattn_w, attn_w = l(\n",
    "                    x, memory, mask=mask, memory_mask=memory_mask, return_att=True\n",
    "                )\n",
    "                selfattn_ws.append(selfattn_w)\n",
    "                attn_ws.append(attn_w)\n",
    "            else:\n",
    "                x = l(\n",
    "                    x, memory, mask=mask, memory_mask=memory_mask, return_att=False\n",
    "                )\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "\n",
    "        if return_att:\n",
    "            selfattn_ws = torch.stack(selfattn_ws, dim=1)\n",
    "            attn_ws = torch.stack(attn_ws, dim=1)\n",
    "            return x, selfattn_ws, attn_ws\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder_config, decoder_config):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = TransformerEncoder(**encoder_config)\n",
    "        #self.decoder = TransformerDecoder(**decoder_config)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\" Forward method\n",
    "\n",
    "        Method used at training time, when the target is known. The target tensor\n",
    "        passed to the decoder is shifted to the right (starting with BOS\n",
    "        symbol). Then, the output of the decoder starts directly with the first\n",
    "        token of the sentence.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Compute the encoder output\n",
    "        encoder_out = self.encoder(src, src_mask)\n",
    "\n",
    "        # TODO: Compute the decoder output\n",
    "        #decoder_out = self.decoder(\n",
    "        #    x=tgt,\n",
    "        #   memory=encoder_out,\n",
    "        #    mask=tgt_mask,\n",
    "        #    memory_mask=src_mask\n",
    "        #)\n",
    "\n",
    "        #return decoder_out\n",
    "        return encoder_out\n",
    "\n",
    "    def generate(self, src, src_mask=None, bos_idx=0, max_len=50):\n",
    "        \"\"\" Generate method\n",
    "\n",
    "        Method used at inference time, when the target is unknown. It\n",
    "        iteratively passes to the decoder the sequence generated so far\n",
    "        and appends the new token to the input again. It uses a Greedy\n",
    "        decoding (argmax).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Compute the encoder output\n",
    "        encoder_out = self.encoder(src, src_mask)\n",
    "\n",
    "        output = torch.LongTensor([bos_idx])\\\n",
    "                    .expand(1, encoder_out.size(1)).to(src.device)\n",
    "        for i in range(max_len):\n",
    "            # TODO: Get the new token\n",
    "            new_token = self.decoder(\n",
    "                x=output,\n",
    "                memory=encoder_out,\n",
    "                memory_mask=src_mask\n",
    "            )[-1].argmax(-1)\n",
    "\n",
    "            output = torch.cat([output, new_token.unsqueeze(0)], dim=0)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        0         1         2         3         4         5         6    \\\n",
       "1      0.0 -0.002737 -0.003256 -0.002842 -0.003326 -0.003696 -0.002624   \n",
       "2      1.0 -0.002686 -0.003358 -0.004155 -0.005550 -0.006590 -0.007223   \n",
       "3      2.0 -0.002638 -0.002471 -0.002312 -0.002172 -0.002040 -0.002214   \n",
       "4      3.0 -0.001875 -0.002034 -0.002197 -0.002201 -0.002347 -0.002576   \n",
       "5      4.0 -0.006637 -0.006698 -0.007560 -0.007685 -0.008237 -0.007881   \n",
       "..     ...       ...       ...       ...       ...       ...       ...   \n",
       "995  994.0 -0.002752 -0.002079 -0.001220 -0.000595  0.000197  0.000827   \n",
       "996  995.0  0.005960  0.005648  0.005329  0.004734  0.004226  0.003602   \n",
       "997  996.0  0.003330  0.003050  0.004500  0.005088  0.005770  0.007044   \n",
       "998  997.0 -0.000866 -0.002570 -0.004254 -0.005894 -0.007288 -0.008403   \n",
       "999  998.0  0.002161  0.003685  0.005081  0.006265  0.007204  0.007833   \n",
       "\n",
       "          7         8         9    ...       247       248       249  \\\n",
       "1   -0.002620 -0.001829 -0.001033  ...  0.003035  0.002601  0.002027   \n",
       "2   -0.008217 -0.007652 -0.007635  ... -0.000154  0.002098  0.003441   \n",
       "3   -0.002414 -0.002673 -0.002983  ...  0.008938  0.007760  0.006023   \n",
       "4   -0.002803 -0.002939 -0.002884  ...  0.009046  0.008281  0.007631   \n",
       "5   -0.006156 -0.006350 -0.005546  ... -0.005254 -0.007157 -0.008638   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.001531  0.002319  0.003068  ...  0.007287  0.007262  0.007102   \n",
       "996  0.002776  0.001995  0.001147  ... -0.006805 -0.006651 -0.006472   \n",
       "997  0.006790  0.007132  0.007646  ...  0.005286  0.004649  0.003885   \n",
       "998 -0.009306 -0.009867 -0.010143  ... -0.007472 -0.005698 -0.003206   \n",
       "999  0.008174  0.008176  0.007942  ...  0.006020  0.006687  0.007170   \n",
       "\n",
       "          250       251       252       253       254       255       256  \n",
       "1    0.001587  0.001841  0.000575  0.001187  0.002046  0.001886  0.002628  \n",
       "2    0.005547  0.006535  0.007792  0.008272  0.008922  0.009115  0.008782  \n",
       "3    0.003708  0.000989 -0.001881 -0.004819 -0.007318 -0.009326 -0.010401  \n",
       "4    0.007210  0.007103  0.007050  0.007189  0.007495  0.007642  0.007892  \n",
       "5   -0.009310 -0.009821 -0.009019 -0.007089 -0.005824 -0.003152 -0.002219  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995  0.007043  0.007077  0.007282  0.007349  0.007484  0.007647  0.008022  \n",
       "996 -0.005976 -0.005619 -0.005147 -0.004462 -0.003799 -0.003172 -0.002490  \n",
       "997  0.003591  0.002326  0.001391  0.000361 -0.000675 -0.001008 -0.000936  \n",
       "998 -0.000475  0.002331  0.004923  0.007142  0.008841  0.009958  0.010485  \n",
       "999  0.007336  0.007188  0.006765  0.006229  0.007247  0.007944  0.008060  \n",
       "\n",
       "[999 rows x 257 columns]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SET_SIZE = 1000\n",
    "\n",
    "data = pd.read_csv(\"./data/dataset_train_2024.csv\", header=None)\n",
    "data = data.iloc[1:TRAIN_SET_SIZE, : ]\n",
    "\n",
    "trainData = data.iloc[:, 258]\n",
    "trainLabels = data.iloc[:, : 257]\n",
    "\n",
    "trainData.head\n",
    "trainLabels.head\n",
    "\n",
    "\n",
    "\n",
    "# def custom_collater(batch):\n",
    "#     \"\"\"\n",
    "#     Custom collater function for batching sequence data.\n",
    "\n",
    "#     Args:\n",
    "#         batch (list of tuples): Each tuple contains (data, label).\n",
    "        \n",
    "#     Returns:\n",
    "#         dict: Batched input data with padding mask.\n",
    "#         dict: Batched target labels with padding mask.\n",
    "#     \"\"\"\n",
    "#     # Extract sequences and labels from the batch\n",
    "#     sequences, labels = zip(*batch)\n",
    "    \n",
    "#     # Convert to PyTorch tensors\n",
    "#     sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "#     labels = [torch.tensor(lbl, dtype=torch.long) for lbl in labels]\n",
    "    \n",
    "#     # Determine max sequence length for padding\n",
    "#     max_seq_len = max([len(seq) for seq in sequences])\n",
    "#     max_label_len = max([len(lbl) for lbl in labels])\n",
    "    \n",
    "#     # Pad sequences and labels\n",
    "#     padded_sequences = torch.zeros(len(sequences), max_seq_len, dtype=torch.long)\n",
    "#     sequence_padding_mask = torch.ones(len(sequences), max_seq_len, dtype=torch.bool)\n",
    "    \n",
    "#     for i, seq in enumerate(sequences):\n",
    "#         padded_sequences[i, :len(seq)] = seq\n",
    "#         sequence_padding_mask[i, :len(seq)] = False\n",
    "    \n",
    "#     padded_labels = torch.zeros(len(labels), max_label_len, dtype=torch.long)\n",
    "#     label_padding_mask = torch.ones(len(labels), max_label_len, dtype=torch.bool)\n",
    "    \n",
    "#     for i, lbl in enumerate(labels):\n",
    "#         padded_labels[i, :len(lbl)] = lbl\n",
    "#         label_padding_mask[i, :len(lbl)] = False\n",
    "\n",
    "#     # Package into dictionaries\n",
    "#     src = {\n",
    "#         \"ids\": padded_sequences,\n",
    "#         \"padding_mask\": sequence_padding_mask,\n",
    "#     }\n",
    "#     tgt = {\n",
    "#         \"ids\": padded_labels,\n",
    "#         \"padding_mask\": label_padding_mask,\n",
    "#     }\n",
    "\n",
    "#     return src, tgt\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_collater' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      3\u001b[0m log_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m numbers_loader_train \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(trainData\u001b[38;5;241m.\u001b[39mvalues, trainLabels\u001b[38;5;241m.\u001b[39mvalues)),\n\u001b[0;32m      8\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m      9\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m---> 10\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39m\u001b[43mcustom_collater\u001b[49m,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m transformer_encoder_cfg \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     19\u001b[0m }\n\u001b[0;32m     20\u001b[0m transformer_decoder_cfg \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     26\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'custom_collater' is not defined"
     ]
    }
   ],
   "source": [
    "lr = 5e-4\n",
    "batch_size = 32\n",
    "log_interval = 50\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "numbers_loader_train = DataLoader(\n",
    "    list(zip(trainData.values, trainLabels.values)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collater,\n",
    ")\n",
    "\n",
    "transformer_encoder_cfg = {\n",
    "    \"num_layers\": 3,\n",
    "    \"embed_dim\": 256,\n",
    "    \"ffn_dim\": 1024,\n",
    "    \"num_heads\": 4,\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "transformer_decoder_cfg = {\n",
    "    \"num_layers\": 3,\n",
    "    \"embed_dim\": 256,\n",
    "    \"ffn_dim\": 1024,\n",
    "    \"num_heads\": 4,\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "model = Transformer(transformer_encoder_cfg, transformer_decoder_cfg)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = F.nll_loss\n",
    "\n",
    "print(\"Training model...\")\n",
    "\n",
    "loss_avg = 0\n",
    "print(numbers_loader_train)\n",
    "for i, (src, tgt) in enumerate(numbers_loader_train):\n",
    "    print(src)\n",
    "    src = {k: v.to(device) for k, v in src.items()}\n",
    "    print(src)\n",
    "    tgt = {k: v.to(device) for k, v in tgt.items()}\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(\n",
    "        src['ids'],\n",
    "        tgt['ids'][:-1],\n",
    "        src['padding_mask'],\n",
    "        tgt['padding_mask'][:, :-1],\n",
    "    )\n",
    "\n",
    "    loss = criterion(\n",
    "        output.reshape(-1, output.size(-1)),\n",
    "        tgt['ids'][1:].flatten()\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_avg += loss.item()\n",
    "    if (i+1) % log_interval == 0:\n",
    "        loss_avg /= log_interval\n",
    "        print(f\"{i+1}/{len(numbers_loader_train)}\\tLoss: {loss_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ricard\\Documents\\Git\\MLEARN\\.venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        # Load data from CSV\n",
    "        data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Extract features\n",
    "        self.sequences_1 = data.iloc[:, 1:129].values  # Columns 1-128 (1-based indexing)\n",
    "        self.sequences_2 = data.iloc[:, 129:257].values  # Columns 129-256\n",
    "        self.extra_feature = data.iloc[:, 257].values  # Column 257\n",
    "        self.features = torch.tensor(\n",
    "            np.hstack([self.sequences_1, self.sequences_2, self.extra_feature.reshape(-1, 1)]),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        # Encode labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = torch.tensor(self.label_encoder.fit_transform(data.iloc[:, -1]), dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "    \n",
    "    def inverseTransform(self, array):\n",
    "        return self.label_encoder.inverse_transform(array)\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, ff_dim, num_classes, dropout=0.3):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, input_dim, embed_dim))  # Match input_dim\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, feature_dim = x.size()\n",
    "        if feature_dim != self.positional_encoding.size(1):\n",
    "            raise ValueError(f\"Feature dimension mismatch: Expected {self.positional_encoding.size(1)}, got {feature_dim}\")\n",
    "\n",
    "        x = self.embedding(x).unsqueeze(1)  # (batch_size, 1, embed_dim)\n",
    "        x += self.positional_encoding[:, :1, :]  # Add positional encoding\n",
    "        x = x.permute(1, 0, 2)  # (seq_len=1, batch_size, embed_dim)\n",
    "        x = self.transformer_encoder(x)  # (seq_len=1, batch_size, embed_dim)\n",
    "        x = x.mean(dim=0)  # (batch_size, embed_dim)\n",
    "        return self.fc(x)  # (batch_size, num_classes)\n",
    "    \n",
    "def f1_loss(y_true, y_pred):\n",
    "    epsilon = 1e-7\n",
    "    tp = (y_true * y_pred).sum(dim=0).float()\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).float()\n",
    "    fp = ((1 - y_true) * y_pred).sum(dim=0).float()\n",
    "    fn = (y_true * (1 - y_pred)).sum(dim=0).float()\n",
    "\n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "    return 1 - f1.mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = self.ce_loss(logits, targets)\n",
    "        pt = torch.exp(-ce_loss)  # Probability of the true class\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "csv_path = \"data/dataset_train_2024.csv\"  # Path to the dataset CSV file\n",
    "batch_size = 16\n",
    "epochs = 3000\n",
    "learning_rate = 1e-4\n",
    "input_dim = 257  # 128+128+1\n",
    "embed_dim = 256\n",
    "num_heads = 4\n",
    "num_layers = 1\n",
    "ff_dim = 4 * embed_dim\n",
    "num_classes = 5  # Adjust based on the dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = CustomDataset(csv_path)\n",
    "train_size = int(0.9 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size  # 20% for testing\n",
    "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = TransformerClassifier(input_dim, embed_dim, num_heads, num_layers, ff_dim, num_classes).to(device)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss(alpha=1, gamma=2)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "#scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#    optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=epochs\n",
    "#)\n",
    "\n",
    "#from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "#scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "----------------------------------------------------------\n",
      "Epoch 1/3000, Loss: 1.0550\n",
      "Validation F1 Score: 0.2016\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 2/3000, Loss: 1.0508\n",
      "Validation F1 Score: 0.2003\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 3/3000, Loss: 1.0468\n",
      "Validation F1 Score: 0.1994\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 4/3000, Loss: 1.0448\n",
      "Validation F1 Score: 0.1988\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 5/3000, Loss: 1.0433\n",
      "Validation F1 Score: 0.1991\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 6/3000, Loss: 1.0448\n",
      "Validation F1 Score: 0.1980\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 7/3000, Loss: 1.0415\n",
      "Validation F1 Score: 0.1986\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 8/3000, Loss: 1.0390\n",
      "Validation F1 Score: 0.1989\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 9/3000, Loss: 1.0380\n",
      "Validation F1 Score: 0.1988\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 10/3000, Loss: 1.0392\n",
      "Validation F1 Score: 0.1992\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 11/3000, Loss: 1.0373\n",
      "Validation F1 Score: 0.1994\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 12/3000, Loss: 1.0399\n",
      "Validation F1 Score: 0.1989\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 13/3000, Loss: 1.0396\n",
      "Validation F1 Score: 0.1986\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 14/3000, Loss: 1.0392\n",
      "Validation F1 Score: 0.1986\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 15/3000, Loss: 1.0385\n",
      "Validation F1 Score: 0.1984\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 16/3000, Loss: 1.0365\n",
      "Validation F1 Score: 0.1982\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 17/3000, Loss: 1.0359\n",
      "Validation F1 Score: 0.1983\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 18/3000, Loss: 1.0342\n",
      "Validation F1 Score: 0.1988\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 19/3000, Loss: 1.0371\n",
      "Validation F1 Score: 0.1990\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 20/3000, Loss: 1.0342\n",
      "Validation F1 Score: 0.1992\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 21/3000, Loss: 1.0356\n",
      "Validation F1 Score: 0.1992\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 22/3000, Loss: 1.0345\n",
      "Validation F1 Score: 0.1994\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 23/3000, Loss: 1.0356\n",
      "Validation F1 Score: 0.1997\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 24/3000, Loss: 1.0346\n",
      "Validation F1 Score: 0.1994\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 25/3000, Loss: 1.0347\n",
      "Validation F1 Score: 0.1996\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 26/3000, Loss: 1.0324\n",
      "Validation F1 Score: 0.1998\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 27/3000, Loss: 1.0350\n",
      "Validation F1 Score: 0.1997\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 28/3000, Loss: 1.0348\n",
      "Validation F1 Score: 0.1997\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 29/3000, Loss: 1.0332\n",
      "Validation F1 Score: 0.1997\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 30/3000, Loss: 1.0332\n",
      "Validation F1 Score: 0.2000\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 31/3000, Loss: 1.0339\n",
      "Validation F1 Score: 0.2000\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 32/3000, Loss: 1.0339\n",
      "Validation F1 Score: 0.2001\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 33/3000, Loss: 1.0333\n",
      "Validation F1 Score: 0.2002\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 34/3000, Loss: 1.0319\n",
      "Validation F1 Score: 0.2005\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 35/3000, Loss: 1.0326\n",
      "Validation F1 Score: 0.2006\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 36/3000, Loss: 1.0321\n",
      "Validation F1 Score: 0.2006\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 37/3000, Loss: 1.0338\n",
      "Validation F1 Score: 0.2007\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 38/3000, Loss: 1.0324\n",
      "Validation F1 Score: 0.2008\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 39/3000, Loss: 1.0315\n",
      "Validation F1 Score: 0.2009\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 40/3000, Loss: 1.0315\n",
      "Validation F1 Score: 0.2009\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 41/3000, Loss: 1.0313\n",
      "Validation F1 Score: 0.2011\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 42/3000, Loss: 1.0318\n",
      "Validation F1 Score: 0.2014\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 43/3000, Loss: 1.0319\n",
      "Validation F1 Score: 0.2014\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 44/3000, Loss: 1.0316\n",
      "Validation F1 Score: 0.2014\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 45/3000, Loss: 1.0309\n",
      "Validation F1 Score: 0.2015\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 46/3000, Loss: 1.0314\n",
      "Validation F1 Score: 0.2015\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 47/3000, Loss: 1.0308\n",
      "Validation F1 Score: 0.2018\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 48/3000, Loss: 1.0323\n",
      "Validation F1 Score: 0.2019\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 49/3000, Loss: 1.0312\n",
      "Validation F1 Score: 0.2020\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 50/3000, Loss: 1.0301\n",
      "Validation F1 Score: 0.2021\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 51/3000, Loss: 1.0308\n",
      "Validation F1 Score: 0.2022\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 52/3000, Loss: 1.0308\n",
      "Validation F1 Score: 0.2024\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 53/3000, Loss: 1.0297\n",
      "Validation F1 Score: 0.2025\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 54/3000, Loss: 1.0310\n",
      "Validation F1 Score: 0.2026\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 55/3000, Loss: 1.0302\n",
      "Validation F1 Score: 0.2028\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 56/3000, Loss: 1.0302\n",
      "Validation F1 Score: 0.2029\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 57/3000, Loss: 1.0298\n",
      "Validation F1 Score: 0.2031\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 58/3000, Loss: 1.0299\n",
      "Validation F1 Score: 0.2033\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 59/3000, Loss: 1.0301\n",
      "Validation F1 Score: 0.2034\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 60/3000, Loss: 1.0292\n",
      "Validation F1 Score: 0.2036\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 61/3000, Loss: 1.0299\n",
      "Validation F1 Score: 0.2037\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 62/3000, Loss: 1.0298\n",
      "Validation F1 Score: 0.2038\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 63/3000, Loss: 1.0299\n",
      "Validation F1 Score: 0.2040\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 64/3000, Loss: 1.0297\n",
      "Validation F1 Score: 0.2041\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 65/3000, Loss: 1.0289\n",
      "Validation F1 Score: 0.2042\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 66/3000, Loss: 1.0296\n",
      "Validation F1 Score: 0.2044\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 67/3000, Loss: 1.0294\n",
      "Validation F1 Score: 0.2045\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 68/3000, Loss: 1.0282\n",
      "Validation F1 Score: 0.2047\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 69/3000, Loss: 1.0293\n",
      "Validation F1 Score: 0.2049\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 70/3000, Loss: 1.0294\n",
      "Validation F1 Score: 0.2050\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 71/3000, Loss: 1.0286\n",
      "Validation F1 Score: 0.2052\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 72/3000, Loss: 1.0292\n",
      "Validation F1 Score: 0.2054\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 73/3000, Loss: 1.0288\n",
      "Validation F1 Score: 0.2055\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 74/3000, Loss: 1.0275\n",
      "Validation F1 Score: 0.2057\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 75/3000, Loss: 1.0301\n",
      "Validation F1 Score: 0.2058\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 76/3000, Loss: 1.0289\n",
      "Validation F1 Score: 0.2060\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 77/3000, Loss: 1.0291\n",
      "Validation F1 Score: 0.2061\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 78/3000, Loss: 1.0293\n",
      "Validation F1 Score: 0.2062\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 79/3000, Loss: 1.0291\n",
      "Validation F1 Score: 0.2064\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 80/3000, Loss: 1.0288\n",
      "Validation F1 Score: 0.2064\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 81/3000, Loss: 1.0291\n",
      "Validation F1 Score: 0.2065\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 82/3000, Loss: 1.0282\n",
      "Validation F1 Score: 0.2067\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 83/3000, Loss: 1.0281\n",
      "Validation F1 Score: 0.2069\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 84/3000, Loss: 1.0288\n",
      "Validation F1 Score: 0.2069\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 85/3000, Loss: 1.0285\n",
      "Validation F1 Score: 0.2070\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 86/3000, Loss: 1.0280\n",
      "Validation F1 Score: 0.2071\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 87/3000, Loss: 1.0284\n",
      "Validation F1 Score: 0.2073\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 88/3000, Loss: 1.0281\n",
      "Validation F1 Score: 0.2075\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 89/3000, Loss: 1.0282\n",
      "Validation F1 Score: 0.2075\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 90/3000, Loss: 1.0282\n",
      "Validation F1 Score: 0.2076\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 91/3000, Loss: 1.0283\n",
      "Validation F1 Score: 0.2078\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 92/3000, Loss: 1.0281\n",
      "Validation F1 Score: 0.2079\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 93/3000, Loss: 1.0282\n",
      "Validation F1 Score: 0.2080\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 94/3000, Loss: 1.0266\n",
      "Validation F1 Score: 0.2082\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 95/3000, Loss: 1.0278\n",
      "Validation F1 Score: 0.2083\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 96/3000, Loss: 1.0285\n",
      "Validation F1 Score: 0.2083\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 97/3000, Loss: 1.0276\n",
      "Validation F1 Score: 0.2085\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 98/3000, Loss: 1.0275\n",
      "Validation F1 Score: 0.2086\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 99/3000, Loss: 1.0276\n",
      "Validation F1 Score: 0.2087\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 100/3000, Loss: 1.0285\n",
      "Validation F1 Score: 0.2088\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 101/3000, Loss: 1.0274\n",
      "Validation F1 Score: 0.2089\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 102/3000, Loss: 1.0275\n",
      "Validation F1 Score: 0.2091\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 103/3000, Loss: 1.0277\n",
      "Validation F1 Score: 0.2092\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 104/3000, Loss: 1.0277\n",
      "Validation F1 Score: 0.2093\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 105/3000, Loss: 1.0280\n",
      "Validation F1 Score: 0.2094\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 106/3000, Loss: 1.0283\n",
      "Validation F1 Score: 0.2096\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 107/3000, Loss: 1.0278\n",
      "Validation F1 Score: 0.2097\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 108/3000, Loss: 1.0283\n",
      "Validation F1 Score: 0.2098\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 109/3000, Loss: 1.0276\n",
      "Validation F1 Score: 0.2099\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 110/3000, Loss: 1.0278\n",
      "Validation F1 Score: 0.2100\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 111/3000, Loss: 1.0277\n",
      "Validation F1 Score: 0.2102\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 112/3000, Loss: 1.0280\n",
      "Validation F1 Score: 0.2102\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 113/3000, Loss: 1.0271\n",
      "Validation F1 Score: 0.2103\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 114/3000, Loss: 1.0274\n",
      "Validation F1 Score: 0.2104\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 115/3000, Loss: 1.0269\n",
      "Validation F1 Score: 0.2104\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 116/3000, Loss: 1.0273\n",
      "Validation F1 Score: 0.2105\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 117/3000, Loss: 1.0272\n",
      "Validation F1 Score: 0.2106\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 118/3000, Loss: 1.0264\n",
      "Validation F1 Score: 0.2107\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 119/3000, Loss: 1.0274\n",
      "Validation F1 Score: 0.2109\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 120/3000, Loss: 1.0281\n",
      "Validation F1 Score: 0.2109\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 121/3000, Loss: 1.0276\n",
      "Validation F1 Score: 0.2110\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 122/3000, Loss: 1.0274\n",
      "Validation F1 Score: 0.2111\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 123/3000, Loss: 1.0275\n",
      "Validation F1 Score: 0.2112\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 124/3000, Loss: 1.0276\n",
      "Validation F1 Score: 0.2113\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 125/3000, Loss: 1.0270\n",
      "Validation F1 Score: 0.2114\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 126/3000, Loss: 1.0272\n",
      "Validation F1 Score: 0.2115\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 127/3000, Loss: 1.0268\n",
      "Validation F1 Score: 0.2116\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 128/3000, Loss: 1.0274\n",
      "Validation F1 Score: 0.2117\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 129/3000, Loss: 1.0276\n",
      "Validation F1 Score: 0.2118\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 130/3000, Loss: 1.0275\n",
      "Validation F1 Score: 0.2119\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 131/3000, Loss: 1.0267\n",
      "Validation F1 Score: 0.2120\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 132/3000, Loss: 1.0272\n",
      "Validation F1 Score: 0.2121\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 133/3000, Loss: 1.0258\n",
      "Validation F1 Score: 0.2121\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 134/3000, Loss: 1.0273\n",
      "Validation F1 Score: 0.2122\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 135/3000, Loss: 1.0266\n",
      "Validation F1 Score: 0.2123\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 136/3000, Loss: 1.0269\n",
      "Validation F1 Score: 0.2124\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 137/3000, Loss: 1.0271\n",
      "Validation F1 Score: 0.2125\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 138/3000, Loss: 1.0270\n",
      "Validation F1 Score: 0.2126\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 139/3000, Loss: 1.0265\n",
      "Validation F1 Score: 0.2127\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 140/3000, Loss: 1.0265\n",
      "Validation F1 Score: 0.2128\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 141/3000, Loss: 1.0270\n",
      "Validation F1 Score: 0.2129\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 142/3000, Loss: 1.0271\n",
      "Validation F1 Score: 0.2130\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 143/3000, Loss: 1.0269\n",
      "Validation F1 Score: 0.2130\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 144/3000, Loss: 1.0265\n",
      "Validation F1 Score: 0.2132\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 145/3000, Loss: 1.0262\n",
      "Validation F1 Score: 0.2132\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 146/3000, Loss: 1.0260\n",
      "Validation F1 Score: 0.2133\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 147/3000, Loss: 1.0268\n",
      "Validation F1 Score: 0.2134\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 148/3000, Loss: 1.0260\n",
      "Validation F1 Score: 0.2135\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 149/3000, Loss: 1.0271\n",
      "Validation F1 Score: 0.2136\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 150/3000, Loss: 1.0269\n",
      "Validation F1 Score: 0.2136\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 151/3000, Loss: 1.0268\n",
      "Validation F1 Score: 0.2137\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 152/3000, Loss: 1.0262\n",
      "Validation F1 Score: 0.2138\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 153/3000, Loss: 1.0262\n",
      "Validation F1 Score: 0.2138\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 154/3000, Loss: 1.0259\n",
      "Validation F1 Score: 0.2139\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 155/3000, Loss: 1.0262\n",
      "Validation F1 Score: 0.2141\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 156/3000, Loss: 1.0261\n",
      "Validation F1 Score: 0.2141\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 157/3000, Loss: 1.0259\n",
      "Validation F1 Score: 0.2142\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 158/3000, Loss: 1.0264\n",
      "Validation F1 Score: 0.2143\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 159/3000, Loss: 1.0262\n",
      "Validation F1 Score: 0.2144\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 160/3000, Loss: 1.0259\n",
      "Validation F1 Score: 0.2145\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 161/3000, Loss: 1.0266\n",
      "Validation F1 Score: 0.2145\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 162/3000, Loss: 1.0266\n",
      "Validation F1 Score: 0.2146\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 163/3000, Loss: 1.0257\n",
      "Validation F1 Score: 0.2147\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 164/3000, Loss: 1.0260\n",
      "Validation F1 Score: 0.2148\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 165/3000, Loss: 1.0254\n",
      "Validation F1 Score: 0.2149\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 166/3000, Loss: 1.0270\n",
      "Validation F1 Score: 0.2149\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 167/3000, Loss: 1.0255\n",
      "Validation F1 Score: 0.2150\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 168/3000, Loss: 1.0249\n",
      "Validation F1 Score: 0.2151\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 169/3000, Loss: 1.0255\n",
      "Validation F1 Score: 0.2152\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 170/3000, Loss: 1.0251\n",
      "Validation F1 Score: 0.2153\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 171/3000, Loss: 1.0247\n",
      "Validation F1 Score: 0.2154\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 172/3000, Loss: 1.0241\n",
      "Validation F1 Score: 0.2156\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 173/3000, Loss: 1.0250\n",
      "Validation F1 Score: 0.2157\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 174/3000, Loss: 1.0238\n",
      "Validation F1 Score: 0.2158\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 175/3000, Loss: 1.0242\n",
      "Validation F1 Score: 0.2159\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 176/3000, Loss: 1.0229\n",
      "Validation F1 Score: 0.2160\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 177/3000, Loss: 1.0227\n",
      "Validation F1 Score: 0.2162\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 178/3000, Loss: 1.0225\n",
      "Validation F1 Score: 0.2163\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 179/3000, Loss: 1.0218\n",
      "Validation F1 Score: 0.2165\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 180/3000, Loss: 1.0208\n",
      "Validation F1 Score: 0.2167\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 181/3000, Loss: 1.0201\n",
      "Validation F1 Score: 0.2168\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 182/3000, Loss: 1.0191\n",
      "Validation F1 Score: 0.2170\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 183/3000, Loss: 1.0178\n",
      "Validation F1 Score: 0.2172\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 184/3000, Loss: 1.0151\n",
      "Validation F1 Score: 0.2174\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 185/3000, Loss: 1.0144\n",
      "Validation F1 Score: 0.2176\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 186/3000, Loss: 1.0107\n",
      "Validation F1 Score: 0.2177\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 187/3000, Loss: 1.0086\n",
      "Validation F1 Score: 0.2179\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 188/3000, Loss: 1.0031\n",
      "Validation F1 Score: 0.2181\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 189/3000, Loss: 0.9924\n",
      "Validation F1 Score: 0.2184\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 190/3000, Loss: 0.9813\n",
      "Validation F1 Score: 0.2187\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 191/3000, Loss: 0.9673\n",
      "Validation F1 Score: 0.2191\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 192/3000, Loss: 0.9494\n",
      "Validation F1 Score: 0.2195\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 193/3000, Loss: 0.9445\n",
      "Validation F1 Score: 0.2199\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 194/3000, Loss: 0.9381\n",
      "Validation F1 Score: 0.2203\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 195/3000, Loss: 0.9312\n",
      "Validation F1 Score: 0.2207\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 196/3000, Loss: 0.9285\n",
      "Validation F1 Score: 0.2211\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 197/3000, Loss: 0.9244\n",
      "Validation F1 Score: 0.2216\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 198/3000, Loss: 0.9253\n",
      "Validation F1 Score: 0.2220\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 199/3000, Loss: 0.9259\n",
      "Validation F1 Score: 0.2224\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 200/3000, Loss: 0.9206\n",
      "Validation F1 Score: 0.2228\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 201/3000, Loss: 0.9168\n",
      "Validation F1 Score: 0.2232\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 202/3000, Loss: 0.9202\n",
      "Validation F1 Score: 0.2236\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 203/3000, Loss: 0.9146\n",
      "Validation F1 Score: 0.2240\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 204/3000, Loss: 0.9247\n",
      "Validation F1 Score: 0.2244\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 205/3000, Loss: 0.9121\n",
      "Validation F1 Score: 0.2248\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 206/3000, Loss: 0.9165\n",
      "Validation F1 Score: 0.2252\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 207/3000, Loss: 0.9095\n",
      "Validation F1 Score: 0.2256\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 208/3000, Loss: 0.9107\n",
      "Validation F1 Score: 0.2260\n",
      "----------------------------------------------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 209/3000, Loss: 0.9121\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "print(\"Training the model...\")\n",
    "y_true, y_pred = [], []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "    print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n",
      "F1 Score: 0.6726\n",
      "Test Accuracy: 0.6733\n"
     ]
    }
   ],
   "source": [
    "# Testing Loop\n",
    "print(\"Testing the model...\")\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        outputs = model(features)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')  # or 'macro', 'micro', depending on your use case\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'predictions_with_indices.csv'\n"
     ]
    }
   ],
   "source": [
    "#Using the model for prediction with the evaluation dataset\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define the dataset class\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.data.iloc[idx].values.astype('float32')  # Adjust for your data type\n",
    "        if self.transform:\n",
    "            inputs = self.transform(inputs)\n",
    "        return inputs\n",
    "\n",
    "# Load the unlabeled dataset\n",
    "csv_path = \"data/dataset_test_no_label_2024.csv\"  # Path to the dataset CSV file\n",
    "unlabeled_df = pd.read_csv(csv_path)  # Update the filename\n",
    "#unlabeled_df = unlabeled_df.iloc[:, :-1]  # Keep all columns except the last\n",
    "unlabeled_df = unlabeled_df.drop(unlabeled_df.columns[0], axis=1)\n",
    "unlabeled_dataset = UnlabeledDataset(unlabeled_df)\n",
    "unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Store predictions and indices\n",
    "predictions = []\n",
    "indices = []\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    for idx, inputs in enumerate(unlabeled_dataloader):\n",
    "        inputs = inputs.to(device)  # Send inputs to the same device as the model\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)  # Get predicted class\n",
    "        \n",
    "        # Save predictions and indices\n",
    "        start_idx = idx * unlabeled_dataloader.batch_size\n",
    "        batch_indices = list(range(start_idx, start_idx + len(inputs)))  # Adjusting the index properly\n",
    "        indices.extend(batch_indices)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Create a DataFrame with indices and predictions\n",
    "output_df = pd.DataFrame({\"ID\": indices, \"MODULATION\": dataset.inverseTransform(predictions)})\n",
    "\n",
    "# Save to a CSV file\n",
    "output_df.to_csv(\"predictions_with_indices.csv\", index=False)\n",
    "\n",
    "print(\"Predictions saved to 'predictions_with_indices.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
