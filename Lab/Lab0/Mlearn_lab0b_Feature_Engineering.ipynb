{"cells":[{"cell_type":"markdown","source":["# **Machine Learning from Data**\n","\n","## Lab 0b: Feature Engineering\n","\n","2024 - Veronica Vilaplana - [GPI @ IDEAI](https://imatge.upc.edu/web/) Research group\n","\n","Based on\n","* Python Data Science Handbook, by Jake VenderPlas, content available in https://github.com/jakevdp/PythonDataScienceHandbook\n"],"metadata":{"id":"YI1RSVgt1pNl"}},{"cell_type":"markdown","metadata":{"id":"G4LZC3lWrPZH"},"source":["# Feature Engineering"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"FhXDfDVQrPZI"},"source":["In most of the following labs we will assume that data is in a tidy format, following the usual Scikit-Learn format as a *feature matrix* `[n_samples, n_features]` . However, in the real world, data rarely comes in such a form.\n","\n","With this in mind, one of the more important steps in using machine learning in practice is *feature engineering*: that is, taking whatever information you have about your problem and turning it into numbers that you can use to build your feature matrix.\n","\n","In this notebook, we will cover a few common examples of feature engineering tasks: we'll look at features for representing categorical data, text, and images.\n","\n","Additionally, we will discuss derived features for increasing model complexity and imputation of missing data.\n","\n","This process is commonly referred to as vectorization, as it involves converting arbitrary data into well-behaved vectors."]},{"cell_type":"markdown","source":["# 1. Feature representation"],"metadata":{"id":"uAhIf-p03cea"}},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"kfYNOHpIrPZJ"},"source":["## 1.1 Categorical Features\n","\n","One common type of nonnumerical data is *categorical* data.\n","For example, assume that we are exploring some data on housing prices, and along with numerical features like \"price\" and \"rooms,\" we also have \"neighborhood\" information.\n","For example, data might look something like this:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"PWxm2m3xrPZJ"},"outputs":[],"source":["data = [\n","    {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},\n","    {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},\n","    {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},\n","    {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}\n","]"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"gyuOczrtrPZK"},"source":["We might be tempted to encode this data with a straightforward numerical mapping:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"TQ0pIzm_rPZK"},"outputs":[],"source":["{'Queen Anne': 1, 'Fremont': 2, 'Wallingford': 3};"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"oallcBdYrPZK"},"source":["But it turns out that this is not generally a useful approach in Scikit-Learn. The package's models make the fundamental assumption that numerical features reflect algebraic quantities, so such a mapping would imply, for example, that *Queen Anne < Fremont < Wallingford*, or even that *Wallingford–Queen Anne = Fremont*, which does not make any sense.\n","\n","In this case, one proven technique is to use *one-hot encoding*, which effectively creates extra columns indicating the presence or absence of a category with a value of 1 or 0, respectively.\n","\n","When the data takes the form of a list of dictionaries, Scikit-Learn's ``DictVectorizer`` performs the one-hot encoding:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"NWwFgkNarPZL"},"outputs":[],"source":["from sklearn.feature_extraction import DictVectorizer\n","vec = DictVectorizer(sparse=False, dtype=int)\n","vec.fit_transform(data)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"-VFhSlDCrPZL"},"source":["Notice that the `neighborhood` column has been expanded into three separate columns representing the three neighborhood labels, and that each row has a 1 in the column associated with its neighborhood.\n","With these categorical features thus encoded, we can proceed as normal with fitting a Scikit-Learn model.\n","\n","To see the meaning of each column, we can inspect the feature names:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"ZYnwFXc9rPZL"},"outputs":[],"source":["vec.get_feature_names_out()"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"gecFj172rPZM"},"source":["There is one clear disadvantage of this approach: if a category has many possible values, this can *greatly* increase the size of the dataset.\n","\n","However, because the encoded data contains mostly zeros, a sparse output can be a very efficient solution:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"g4LjqyQ0rPZM"},"outputs":[],"source":["vec = DictVectorizer(sparse=True, dtype=int)\n","vec.fit_transform(data)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"fpUjtXDYrPZM"},"source":["Nearly all of the Scikit-Learn estimators accept such sparse inputs when fitting and evaluating models. `sklearn.preprocessing.OneHotEncoder` and `sklearn.feature_extraction.FeatureHasher` are two additional tools that Scikit-Learn includes to support this type of encoding (check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html))."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"fCi2J8dbrPZM"},"source":["## 1.2 Text Features\n","\n","Another common need in feature engineering is to convert text to a set of representative numerical values.\n","For example, most automatic mining of social media data relies on some form of encoding the text as numbers.\n","One of the simplest methods of encoding this type of data is by *word counts*: you take each snippet of text, count the occurrences of each word within it, and put the results in a table.\n","\n","For example, consider the following set of three phrases:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"tags":[],"id":"q7LnLVXBrPZM"},"outputs":[],"source":["sample = ['problem of evil',\n","          'evil queen',\n","          'horizon problem']"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"abb2jtUPrPZM"},"source":["For a vectorization of this data based on word count, we could construct individual columns representing the words \"problem,\" \"of,\" \"evil,\" and so on.\n","While doing this by hand would be possible for this simple example, for a large number of words we can use using Scikit-Learn's `CountVectorizer`:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"wyd5rnUjrPZM"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vec = CountVectorizer()\n","X = vec.fit_transform(sample)\n","X"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"MaOQUpC4rPZM"},"source":["The result is a sparse matrix recording the number of times each word appears; it is easier to inspect if we convert this to a `DataFrame` with labeled columns:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"FZTRizZ3rPZM"},"outputs":[],"source":["import pandas as pd\n","pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"uGu82KZDrPZM"},"source":["There are some issues with using a simple raw word count, however: it can lead to features that put too much weight on words that appear very frequently, and this can be suboptimal in some classification algorithms.\n","One approach to fix this is known as *term frequency–inverse document frequency* (*TF–IDF*), which weights the word counts by a measure of how often they appear in the documents.\n","The syntax for computing these features is similar to the previous example:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"RS4f-iSlrPZM"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vec = TfidfVectorizer()\n","X = vec.fit_transform(sample)\n","pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"3w18uA3xrPZN"},"source":["## 1.3 Image Features\n","\n","Another common need is to suitably encode images for machine learning analysis.\n","The simplest approach is using the pixel values themselves. But depending on the application, such an approach may not be optimal.\n","\n","Another posibility is to find a suitable representation, using for example the Bag of words model or HOG features (histogram of gradient orientation). The topic is not trivial and it is beyond the scope of this lab.\n"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"gyZAPCNWrPZN"},"source":["# 2. Derived Features\n","\n","Another useful type of feature is one that is mathematically derived from some input features. A typical example is the construction of *polynomial features* from our input data.\n","\n","For example, this data clearly cannot be well described by a straight line:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"UOJtqsRyrPZN"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","x = np.array([1, 2, 3, 4, 5])\n","y = np.array([4, 2, 1, 3, 7])\n","plt.scatter(x, y);"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Uw35WIYprPZN"},"source":["We can still fit a line to the data using `LinearRegression` and get the optimal result, as shown in Figure 40-2:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"_q2YzwihrPZN"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","X = x[:, np.newaxis]\n","model = LinearRegression().fit(X, y)\n","yfit = model.predict(X)\n","plt.scatter(x, y)\n","plt.plot(x, yfit);"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"NATyjYEdrPZN"},"source":["But it's clear that we need a more sophisticated model to describe the relationship between $x$ and $y$.\n","\n","One approach to this is to transform the data, adding extra columns of features to drive more flexibility in the model.\n","For example, we can add polynomial features to the data this way:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"HDx0R_CQrPZN"},"outputs":[],"source":["from sklearn.preprocessing import PolynomialFeatures\n","poly = PolynomialFeatures(degree=3, include_bias=False)\n","X2 = poly.fit_transform(X)\n","print(X2)"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"1PGnwc9xrPZN"},"source":["The derived feature matrix has one column representing $x$, a second column representing $x^2$, and a third column representing $x^3$.\n","Computing a linear regression on this expanded input gives a much closer fit to our data, as you can see in Figure 40-3:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"SPTHmWSLrPZN"},"outputs":[],"source":["model = LinearRegression().fit(X2, y)\n","yfit = model.predict(X2)\n","plt.scatter(x, y)\n","plt.plot(x, yfit);"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"I3Rey0UPrPZN"},"source":["This idea of improving a model not by changing the model, but by transforming the inputs, is fundamental to many of the more powerful machine learning methods.\n","More generally, this is one motivational path to the powerful set of techniques known as *kernel methods*."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"WpX6FljbrPZN"},"source":["# 3. Imputation of Missing Data\n","\n","Another common need in feature engineering is handling of missing data.\n","For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with many scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning.\n","\n","A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data.\n","\n"]},{"cell_type":"markdown","source":["One type of imputation algorithm is univariate, which imputes values in the i-th feature dimension using only non-missing values in that feature dimension (e.g. SimpleImputer). By contrast, multivariate imputation algorithms use the entire set of available feature dimensions to estimate the missing values (e.g. IterativeImputer)."],"metadata":{"id":"rv3djwpiMdM4"}},{"cell_type":"markdown","source":["##3.1 Univariate feature imputation\n","\n","The `SimpleImputer` class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.\n","\n","The following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean value of the columns that contain the missing values:"],"metadata":{"id":"7p8bZr4-M1y3"}},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"vjm3eDi1rPZN"},"outputs":[],"source":["from numpy import nan\n","X = np.array([[ nan, 0,   3  ],\n","              [ 3,   7,   9  ],\n","              [ 3,   5,   2  ],\n","              [ 4,   nan, 6  ],\n","              [ 8,   8,   1  ]])\n","y = np.array([14, 16, -1,  8, -5])"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"wK_u66lgrPZN"},"outputs":[],"source":["from sklearn.impute import SimpleImputer\n","imp = SimpleImputer(strategy='mean')\n","X2 = imp.fit_transform(X)\n","X2"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"yXtFFlhHrPZN"},"source":["We see that in the resulting data, the two missing values have been replaced with the mean of the remaining values in the column. This imputed data can then be fed directly into, for example, a `LinearRegression` estimator:"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":true,"editable":true,"jupyter":{"outputs_hidden":false},"id":"QNFo7sHNrPZN"},"outputs":[],"source":["model = LinearRegression().fit(X2, y)\n","model.predict(X2)"]},{"cell_type":"markdown","source":["The SimpleImputer class also supports categorical data represented as string values or pandas categoricals when using the 'most_frequent' or 'constant' strategy:\n","\n"],"metadata":{"id":"RGXOXy4FOO7m"}},{"cell_type":"code","source":["import pandas as pd\n","df = pd.DataFrame([[\"a\", \"x\"],\n","                   [np.nan, \"y\"],\n","                   [\"a\", np.nan],\n","                   [\"b\", \"y\"]], dtype=\"category\")\n","\n","imp = SimpleImputer(strategy=\"most_frequent\")\n","print(imp.fit_transform(df))"],"metadata":{"id":"fkTKdaTyOQKh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.2 Multivariate feature imputation\n","\n","A more sophisticated approach is to use the IterativeImputer class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned."],"metadata":{"id":"m3bGg78BPAGA"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","imp = IterativeImputer(max_iter=10, random_state=0)\n","imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\n","IterativeImputer(random_state=0)\n","X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\n","# the model learns that the second feature is double the first\n","print(np.round(imp.transform(X_test)))"],"metadata":{"id":"r26trI2KPSbe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["NOTE: In the statistics community, it is common practice to perform multiple imputations, generating, for example, m separate imputations for a single feature matrix. Each of these m imputations is then put through the subsequent analysis pipeline (e.g. feature engineering, clustering, regression, classification). The m final analysis results (e.g. held-out validation errors) allow the data scientist to obtain understanding of how analytic results may differ as a consequence of the inherent uncertainty caused by the missing values. The above practice is called multiple imputation."],"metadata":{"id":"ydxIgZqXPtI1"}},{"cell_type":"markdown","source":["## 3.3 Nearest Neighbor imputation\n","\n","The KNNImputer class provides imputation for filling in missing values using the k-Nearest Neighbors approach. By default, a euclidean distance metric that supports missing values, nan_euclidean_distances, is used to find the nearest neighbors. Each missing feature is imputed using values from n_neighbors nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor."],"metadata":{"id":"604N5AJFP36r"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.impute import KNNImputer\n","nan = np.nan\n","X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\n","imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n","imputer.fit_transform(X)"],"metadata":{"id":"YoHVq43PQgLi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check more details on imputation [here](https://scikit-learn.org/stable/modules/impute.html).\n","\n","For an example on usage, see [Imputing missing values before building an estimator](https://scikit-learn.org/stable/auto_examples/impute/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py).\n","\n"],"metadata":{"id":"K4dQgXIlOxUq"}}],"metadata":{"anaconda-cloud":{},"jupytext":{"formats":"ipynb,md"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"colab":{"provenance":[{"file_id":"https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.04-Feature-Engineering.ipynb","timestamp":1723585149603}]}},"nbformat":4,"nbformat_minor":0}