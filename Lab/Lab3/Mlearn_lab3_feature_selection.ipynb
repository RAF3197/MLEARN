{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Machine Learning from Data**\n","\n","## Lab 3: Feature Selection\n","\n","2024 - Veronica Vilaplana - [GPI @ IDEAI](https://imatge.upc.edu/web/) Research group\n","\n","Based on\n","* Scikit-Learn documentation and examples\n","* [A short guide for feature engineering and feature selection](https://github.com/Yimeng-Zhang/feature-engineering-and-feature-selection/blob/master/A%20Short%20Guide%20for%20Feature%20Engineering%20and%20Feature%20Selection.pdf), by Yimeng-Zhang\n","* [A feature selection tutorial](https://github.com/slowvak/FeatureSelectionTutorial/blob/main/FeatureSelection.ipynb)\n"],"metadata":{"id":"oWDTYmQoUu7P"}},{"cell_type":"markdown","source":["#Feature Selection\n","\n","Feature selection is the process of identifying and selecting a subset of relevant features for use in machine learning model building. Contrary to the belief that \"more data is always better,\" including irrelevant or redundant features can actually hinder model performance.\n","\n","By employing feature selection, we can:\n","\n","* Simplify models: Leading to improved interpretability.\n","* Reduce training time and computational cost.\n","* Lower data collection costs.\n","* Mitigate the curse of dimensionality.\n","* Enhance generalization by reducing overfitting.\n","\n","It's important to note that the optimal feature subset can vary depending on the specific machine learning algorithm. Therefore, feature selection should be considered an integral part of the model building process, rather than a separate step. For example, when selecting features for a linear model, techniques like regression coefficient importance or Lasso regularization are particularly effective. For tree-based models, tree-derived feature importance can be a valuable tool.\n","\n","\n","Feature selectio methods can be broadly classified into **unsupervised** or **supervised methods**\n","\n","\n","**Unsupervised methods** are such methods that do not make use of any labels. In other words, they don’t need access to the target variable of the machine learning model.\n","We might want to discard the features with:\n","* Zero or near-zero variance. Features that are (almost) constant provide little information to learn from and thus are irrelevant.\n","* Many missing values. While dropping incomplete features is not the preferred way to handle missing data, it is often a good start, and if too many entries are missing, it might be the only sensible thing to do since such features are likely inconsequential.\n","* High multicollinearity; multicollinearity means a strong correlation between different features, which might signal redundancy issues. This method eliminates highly correlated features to reduce redundancy.\n","\n","In turn, **supervised methods** can be grouped into  **filter methods**,  **wrapper methods** and **embedded methods**\n","\n","\n","\n","**Filter methods:** select features based on a performance measure regardless of the ML algorithm later employed.  Univariate filters evaluate and rank a single feature according to a certain criteria, while multivariate filters evaluate the entire feature space. As a result, filter methods are suited for a first step quick screen and removal of irrelevant features.\n","\n","Most common filter methods are:\n","* Univariate statistical tests: Compute chi-squared tests for categorial features and ANOVA for numerical features\n","* Mutual information filter: Mutual information measures how much information the presence/absence of a feature contributes to making the correct prediction of the target variable.\n","\n","**Wrapper methods:**  use a search strategy to search through the space of possible feature subsets and evaluate each subset by the quality of the performance on a ML algorithm. Practically any combination of search strategy\n","and algorithm can be used as a wrapper.\n","\n","The most common search strategy group is Sequential search, including Forward Selection and Backward Elimination, or Recursive Feature Elimination.\n","\n","Another key element in wrappers is stopping criteria. When to stop the search? In general there're three: performance increase, performance decrease or\n","predefined number of features is reached.\n","\n","For example, *Forward Feature Selection* starts with an empty set and iteratively adds the most informative feature. It starts by evaluating all features individually and selects the one that generates the best performing algorithm, according to a pre-set evaluation criteria. In the second step, it evaluates all possible combinations of the selected feature and a second feature, and selects the pair that produce the best performing algorithm based on the same pre-set criteria. The pre-set criteria can be the roc_auc for classification and the r squared for regression for example.\n","This selection procedure evaluates all possible single, double, triple and so on feature combinations. Therefore, it is quite computationally expensive, and sometimes, if feature space is big, even unfeasible.\n","\n","*Backward Feature Elimination* starts with the full set of features and iteratively removes the least informative feature.\n","\n","*Recursive Feature Elimination (RFE)* recursively removes features based on their importance scores from a model.\n","\n","**Embedded Methods:** combine the advantages of the filter and wrapper methods. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification at same\n","time. Common embedded methods include\n","\n","*Lasso*, that penalizes the model's coefficients, effectively shrinking less important features towards zero and various types of\n","\n","*Tree-based algorithms*, where feature importance scores can be derived from the tree structure.\n","\n","They are less computationally expensive as they only train the model once, compared to Wrappers.\n","\n","\n","The following are the feature selection strategies implemented in Scikit-Learn.\n","\n","There are other libraries with more methods, for example\n","* [Py-FS](https://pypi.org/project/Py-FS/)\n","* [Feature engine](https://feature-engine.trainindata.com/en/latest/)\n","* [Feature engineering and feature selection](https://github.com/Yimeng-Zhang/feature-engineering-and-feature-selection/blob/master/4.1_Demo_Feature_Selection_Filter.ipynb)"],"metadata":{"id":"J94AOwn4VKr0"}},{"cell_type":"markdown","source":["## 1. Removing features with low variance\n","\n","`VarianceThreshold` is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples.\n","\n","As an example, suppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is given by\n","\n","$Var[X]= p(1-p)$\n","\n","so we can select using the threshold .8 * (1 - .8):"],"metadata":{"id":"eKT3k1ERVfOq"}},{"cell_type":"code","source":["from sklearn.feature_selection import VarianceThreshold\n","X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n","sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n","sel.fit_transform(X)"],"metadata":{"id":"kioupXfAWFlY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731349659912,"user_tz":-60,"elapsed":8567,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}},"outputId":"9f86e3cd-ca8b-4423-bb8f-15152794b038"},"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 1],\n","       [1, 0],\n","       [0, 0],\n","       [1, 1],\n","       [1, 0],\n","       [1, 1]])"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["## 2. Univariate feature selection\n","\n","Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the transform method:\n","\n","* `SelectKBest` removes all but the\n"," highest scoring features\n","\n","* `SelectPercentile` removes all but a user-specified highest scoring percentage of features using common univariate statistical tests for each feature: false positive rate SelectFpr, false discovery rate SelectFdr, or family wise error SelectFwe.\n","\n","* `GenericUnivariateSelect` allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator.\n","\n","For instance, we can use a F-test to retrieve the two best features for a dataset as follows:"],"metadata":{"id":"DimaqOcUWmNo"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import f_classif\n","X, y = load_iris(return_X_y=True)\n","print(X.shape)\n","X_new = SelectKBest(f_classif, k=2).fit_transform(X, y)\n","print(X_new.shape)"],"metadata":{"id":"enktzKj4W6ob","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731349660657,"user_tz":-60,"elapsed":748,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}},"outputId":"2d015333-e519-4212-8153-5dede760fd51"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["(150, 4)\n","(150, 2)\n"]}]},{"cell_type":"markdown","source":["These objects take as input a scoring function that returns univariate scores and p-values (or only scores for SelectKBest and SelectPercentile):\n","\n","* For regression: r_regression, f_regression, mutual_info_regression\n","\n","* For classification: chi2, f_classif, mutual_info_classif\n","\n","The methods based on F-test estimate the degree of linear dependency between two random variables.\n","\n","\n","This [notebook](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html#sphx-glr-auto-examples-feature-selection-plot-feature-selection-py) is an example of using univariate feature selection to improve classification accuracy on a noisy dataset (using Support Vector Machines)."],"metadata":{"id":"afmYd9pHXeo3"}},{"cell_type":"markdown","source":["## 3. Recursive feature elimination\n","\n","Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute (such as coef_, feature_importances_) or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n","\n","See for example [Recursive feature elimination](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_digits.html#sphx-glr-auto-examples-feature-selection-plot-rfe-digits-py): A recursive feature elimination example showing the relevance of pixels in a digit classification task."],"metadata":{"id":"m-lRW22SYUGb"}},{"cell_type":"markdown","source":["## 4. Feature selection using SelectFromModel\n","\n","`SelectFromModel` is a meta-transformer that can be used alongside any estimator that assigns importance to each feature through a specific attribute. The features are considered unimportant and removed if the corresponding importance of the feature values are below the provided threshold parameter.\n","\n","For example, we can use the **L1-based Feature selection**\n","\n","Linear models penalized with the L1 norm have sparse solutions: many of their estimated coefficients are zero. When the goal is to reduce the dimensionality of the data to use with another classifier, they can be used along with `SelectFromModel` to select the non-zero coefficients. In particular, sparse estimators useful for this purpose are the `Lasso` for regression, and of `LogisticRegression` and `LinearSVC` for classification:\n"],"metadata":{"id":"if3CPls0YZF_"}},{"cell_type":"code","source":["from sklearn.svm import LinearSVC\n","from sklearn.datasets import load_iris\n","from sklearn.feature_selection import SelectFromModel\n","X, y = load_iris(return_X_y=True)\n","print(X.shape)\n","lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n","model = SelectFromModel(lsvc, prefit=True)\n","X_new = model.transform(X)\n","print(X_new.shape)"],"metadata":{"id":"2KJE3CmCZQwx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731349660658,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}},"outputId":"4b2bd9e0-08e3-4bfe-df73-50715b539b3f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["(150, 4)\n","(150, 3)\n"]}]},{"cell_type":"markdown","source":["With SVMs and logistic-regression, the parameter C controls the sparsity: the smaller C the fewer features selected. With Lasso, the higher the alpha parameter, the fewer features selected.\n","\n","\n","See more information on Scikit-Learn feature selection methods [here](https://scikit-learn.org/stable/modules/feature_selection.html)."],"metadata":{"id":"brz_sYhbZbGp"}},{"cell_type":"markdown","source":["## 5. Examples. Feature selection on the Pima Indians Diabetes Dataset"],"metadata":{"id":"dFptQtqUDusn"}},{"cell_type":"markdown","source":["We will try some feature selection methods using the Pima Indians Diabetes Dataset"],"metadata":{"id":"5s37ibCu5om0"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np"],"metadata":{"id":"APH46_BJvoLg","executionInfo":{"status":"ok","timestamp":1731349660658,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["First, we need to get the data. Upload the file 'diabetes.csv'"],"metadata":{"id":"a7vLzi8s5y4k"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","data = pd.read_csv(\"drive/MyDrive/MLEARN/Lab/Lab3/diabetes.csv\")\n","# print out the first few lines to make sure we got the data and that it looks reasonable\n","data.head()"],"metadata":{"id":"bG51NtvSvsuZ","colab":{"base_uri":"https://localhost:8080/","height":376},"executionInfo":{"status":"error","timestamp":1731349782288,"user_tz":-60,"elapsed":121634,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}},"outputId":"c1e6e740-5bb9-4729-a010-e54f264adb32"},"execution_count":5,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"mount failed","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-8d85321ba010>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/MyDrive/MLEARN/Lab/Lab3/diabetes.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print out the first few lines to make sure we got the data and that it looks reasonable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed"]}]},{"cell_type":"markdown","source":["Next we import modules for visualizing data and making plots. Here will do a quick plot to see the correlation coefficients of the various features in the data."],"metadata":{"id":"DjPL_z1nwj7v"}},{"cell_type":"code","source":["import matplotlib\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#Lets look at how much the various features are correlated using Pearson Correlation\n","plt.figure(figsize=(12,10))\n","cor = data.corr()\n","sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n","plt.show()"],"metadata":{"id":"p2uXesyMwjGo","executionInfo":{"status":"aborted","timestamp":1731349782289,"user_tz":-60,"elapsed":11,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next we convert the dataframe into a set of feature vectors 'X' and the class or label which we will call 'y', which in our case is 1-diabetic 0-not diabetic."],"metadata":{"id":"cMyVpWTLws-Y"}},{"cell_type":"code","source":["array = data.values\n","X = array[:,0:8]\n","y = array[:,8]\n","\n","#print out the shape of the X feature array--number of features and number of rows/examples\n","X.shape"],"metadata":{"id":"bws3bBjOwtKY","executionInfo":{"status":"aborted","timestamp":1731349782289,"user_tz":-60,"elapsed":11,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will train a linear classifier (LDA), and compute baseline accuracy and error with all the features."],"metadata":{"id":"xVFDVR6F6ttp"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix"],"metadata":{"id":"570IqiYbxZyq","executionInfo":{"status":"aborted","timestamp":1731349782289,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=109) # 70% training and 30% test\n","\n","lda = LinearDiscriminantAnalysis(solver=\"svd\",store_covariance=True)\n","\n","ldamodel = lda.fit(X_train, y_train)\n","y_tpred_lda = ldamodel.predict(X_train)\n","y_testpred_lda = ldamodel.predict(X_test)\n","\n","lda_train_accuracy = accuracy_score(y_train,y_tpred_lda)\n","lda_train_error = 1. - lda_train_accuracy\n","print('LDA train accuracy: %f' %lda_train_accuracy)\n","print('LDA train error: %f' %lda_train_error)\n","print('LDA train confusion matrix:')\n","print(confusion_matrix(y_train,y_tpred_lda))\n","\n","\n","lda_test_accuracy = accuracy_score(y_test,y_testpred_lda)\n","lda_test_error = 1. - lda_train_accuracy\n","print('LDA test accuracy: %f' %lda_test_accuracy)\n","print('LDA test error: %f' %lda_test_error)\n","print('LDA test confusion matrix:')\n","print(confusion_matrix(y_test,y_testpred_lda))\n"],"metadata":{"id":"hG7cpQ3lxotB","executionInfo":{"status":"aborted","timestamp":1731349782289,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we will use some feature selection methods.\n","We start with filter method SelectKBest, based on a F-test that estimates the degree of linear dependency between two random variables (feature anf target)."],"metadata":{"id":"6IKYxxIm78IN"}},{"cell_type":"code","source":["from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import f_classif\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=109) # 70% training and 30% test\n","\n","# Feature extraction--select the 4 best features\n","test = SelectKBest(score_func=f_classif, k=5)\n","fit = test.fit(X_train, y_train)\n","\n","# Summarize scores\n","np.set_printoptions(precision=3)\n","feature_names = data.columns[0:8]\n","\n","#print (feature_names) and (fit.scores_)\n","for i, feature in enumerate (feature_names):\n","    print (f'{fit.scores_[i]}, {feature}')\n"],"metadata":{"id":"uiD73XnhyIwp","executionInfo":{"status":"aborted","timestamp":1731349782289,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's sort the list of values\n"],"metadata":{"id":"RTFTa8GL8Z5A"}},{"cell_type":"code","source":["zipped_lists = zip(fit.scores_, feature_names)\n","sorted_pairs = sorted(zipped_lists)\n","for f, val in sorted_pairs:\n","    print (f'{f} - {val}')"],"metadata":{"id":"IQ525lO3ynrD","executionInfo":{"status":"aborted","timestamp":1731349782289,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we will train the linear classifier using only the last five features. We first remove the features from the whole set and then split it again into train an test subsets"],"metadata":{"id":"A5K6Fo-k87AE"}},{"cell_type":"code","source":["#First, need to remove the 'Outcome' column since that is the target\n","fclas_df = data.drop(columns=[\"Outcome\"])\n","\n","# delete the columns we don't want\n","fclas_df = fclas_df.drop(columns=[\"BloodPressure\",\"SkinThickness\",\"Insulin\"])\n","\n","d_fclas = fclas_df.values\n","X_1 = d_fclas[:,:]\n"],"metadata":{"id":"XmfN8iTyyt-s","executionInfo":{"status":"aborted","timestamp":1731349782289,"user_tz":-60,"elapsed":9,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X_1, y, test_size=0.3, random_state=109) # 70% training and 30% test\n","\n","lda = LinearDiscriminantAnalysis(solver=\"svd\",store_covariance=True)\n","\n","ldamodel = lda.fit(X_train, y_train)\n","y_tpred_lda = ldamodel.predict(X_train)\n","y_testpred_lda = ldamodel.predict(X_test)\n","\n","\n","lda_train_accuracy = accuracy_score(y_train,y_tpred_lda)\n","lda_train_error = 1. - lda_train_accuracy\n","print('LDA train accuracy: %f' %lda_train_accuracy)\n","print('LDA train error: %f' %lda_train_error)\n","print('LDA train confusion matrix:')\n","print(confusion_matrix(y_train,y_tpred_lda))\n","\n","\n","lda_test_accuracy = accuracy_score(y_test,y_testpred_lda)\n","lda_test_error = 1. - lda_train_accuracy\n","print('LDA test accuracy: %f' %lda_test_accuracy)\n","print('LDA test error: %f' %lda_test_error)\n","print('LDA test confusion matrix:')\n","print(confusion_matrix(y_test,y_testpred_lda))\n","\n","\n","print (f'Accuracy on the test set with the best 5 features using F-test is {lda_test_accuracy}')\n"],"metadata":{"id":"ZcCgkqSoziYU","executionInfo":{"status":"aborted","timestamp":1731349782290,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### <font color='red'>TO_DO: try adding or removing features and see the impact on performance</font>\n","\n"],"metadata":{"id":"fSpCF07q-eC0"}},{"cell_type":"markdown","source":["### Next we will apply a wrapper method--Recursive Feature Elimination or RFE\n","\n"],"metadata":{"id":"OYI2xZCJ0Sax"}},{"cell_type":"code","source":["# Split original data (with all features)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=109) # 70% training and 30% test\n","\n","# Import your necessary dependencies\n","from sklearn.feature_selection import RFE\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","\n","\n","estimator = LinearDiscriminantAnalysis(solver=\"svd\",store_covariance=True)\n","\n","# we will ask for the top 5 features\n","selector = RFE(estimator, n_features_to_select=5, step=1)\n","selector = selector.fit(X_train, y_train)\n","print(selector.support_)\n","rank=selector.ranking_\n","print (\"Or if you want to see the ranking of those that didn't make the cut...\")\n","print(rank)"],"metadata":{"id":"0-V6wgw30UqC","executionInfo":{"status":"aborted","timestamp":1731349782290,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We remove the features and train the classifier"],"metadata":{"id":"Zo7br51j_pHp"}},{"cell_type":"code","source":["# drop all columns not '1'\n","X_1 = data.drop(data.columns[8], axis=1)  # get rid of 'class'\n","for i in range(7):\n","    if selector.support_[i] == False:\n","        X_1 = X_1.drop(X_1.columns[i], axis=1)"],"metadata":{"id":"g3O_kO_Q1IWV","executionInfo":{"status":"aborted","timestamp":1731349782290,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split the dataset X1 (using only 4 features)\n","X_train, X_test, y_train, y_test = train_test_split(X_1, y, test_size=0.3, random_state=109) # 70% training and 30% test\n","\n","lda = LinearDiscriminantAnalysis(solver=\"svd\",store_covariance=True)\n","\n","ldamodel = lda.fit(X_train, y_train)\n","y_tpred_lda = ldamodel.predict(X_train)\n","y_testpred_lda = ldamodel.predict(X_test)\n","\n","\n","lda_train_accuracy = accuracy_score(y_train,y_tpred_lda)\n","lda_train_error = 1. - lda_train_accuracy\n","print('LDA train accuracy: %f' %lda_train_accuracy)\n","print('LDA train error: %f' %lda_train_error)\n","print('LDA train confusion matrix:')\n","print(confusion_matrix(y_train,y_tpred_lda))\n","\n","\n","lda_test_accuracy = accuracy_score(y_test,y_testpred_lda)\n","lda_test_error = 1. - lda_train_accuracy\n","print('LDA test accuracy: %f' %lda_test_accuracy)\n","print('LDA test error: %f' %lda_test_error)\n","print('LDA test confusion matrix:')\n","print(confusion_matrix(y_test,y_testpred_lda))\n","\n","\n","print (f'Test accuracy with best 5 features using RFE is {lda_test_accuracy}')\n"],"metadata":{"id":"jaB3HZYp1JSQ","executionInfo":{"status":"aborted","timestamp":1731349782291,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally we will use the same method, varying the number of selected features:"],"metadata":{"id":"kCHzSX1YAFfD"}},{"cell_type":"code","source":["# split the original dataset with all features\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=109) # 70% training and 30% test\n","\n","# nof - number of features that we have\n","nof=8\n","nof_list=np.arange(1,nof+1)\n","high_score=0\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","\n","score_list =[]\n","best_nof = 0\n","for n in range(nof-1):\n","    model = LinearDiscriminantAnalysis(solver=\"svd\",store_covariance=True)\n","\n","    rfe = RFE(model,n_features_to_select=nof_list[n])\n","    X_train_rfe = rfe.fit_transform(X_train,y_train)\n","    X_test_rfe = rfe.transform(X_test)\n","    model.fit(X_train_rfe,y_train)\n","    score = model.score(X_test_rfe,y_test)\n","\n","    score_list.append(score)\n","    print(f\" case: {n}, best {best_nof} features, scoring {high_score}\")\n","    if(score>high_score):\n","        high_score = score\n","        best_nof = nof_list[n]\n","\n","print(f\"  Optimal case: {best_nof} features, scoring {high_score}\")"],"metadata":{"id":"5rRegrp51hkN","executionInfo":{"status":"aborted","timestamp":1731349782291,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}}},"execution_count":null,"outputs":[]}]}