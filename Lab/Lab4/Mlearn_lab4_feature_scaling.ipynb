{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Machine Learning from Data**\n","\n","## Lab 4b: Feature Scaling\n","\n","2024 - Veronica Vilaplana - [GPI @ IDEAI](https://imatge.upc.edu/web/) Research group\n","\n","Based on\n","* Scikit-Learn documentation and examples\n","* [A short guide for feature engineering and feature selection](https://github.com/Yimeng-Zhang/feature-engineering-and-feature-selection/blob/master/A%20Short%20Guide%20for%20Feature%20Engineering%20and%20Feature%20Selection.pdf), by Yimeng-Zhang"],"metadata":{"id":"Pu3c4wFyhze6"}},{"cell_type":"markdown","source":["# Feature scaling\n","\n","Feature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the\n","data preprocessing step.\n","\n","\n","Why Feature Scaling Matters?\n","\n","* If range of inputs varies, in some algorithms, object functions will not work properly.\n","* Gradient descent converges much faster with feature scaling done. Gradient descent is a common optimization algorithm used in logistic regression, SVMs, neural networks etc.\n","* Algorithms that involve distance calculation like KNN or Clustering are also affected by the magnitude of the features. Just consider how Euclidean distance is calculated: taking the square root of the sum of\n","the squared differences between observations. This distance can be greatly affected by differences in scale among the variables. Variables with large variances have a larger effect on this measure than variables with small variances.\n","\n","Note: Tree-based algorithms are almost the only algorithms that are not affected by the magnitude of the input, as we can easily see from how trees are built. When deciding how to make a split, tree algorithm look\n","for decisions like \"whether feature value X>3.0\" and compute the purity of the child node after the split, so the scale of the feature does not count."],"metadata":{"id":"klfAGibDjT1h"}},{"cell_type":"markdown","source":["## 1. Standardization\n","\n","Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n","\n","In practice we often ignore the shape of the distribution and just transform the data to center it by **removing the mean value of each feature**, then scale it by **dividing non-constant features by their standard deviation**.\n","\n","For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) may assume that all features are centered around zero or have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n","\n","In Scikit-Learn, the preprocessing module provides the `StandardScaler` utility class, which is a quick and easy way to perform the following operation on an array-like dataset. This class implements the Transformer API to compute the mean and standard deviation on a training set so as to be able to later re-apply the same transformation on the testing set. This class is hence suitable for use in the early steps of a Pipeline:"],"metadata":{"id":"8HWj0kzelPUD"}},{"cell_type":"code","source":["from sklearn import preprocessing\n","import numpy as np\n","X_train = np.array([[ 1., -1.,  2.],\n","                    [ 2.,  0.,  0.],\n","                    [ 0.,  1., -1.]])\n","scaler = preprocessing.StandardScaler().fit(X_train)\n","print('scaler.mean=', scaler.mean_)\n","\n","print('scaler.scale=', scaler.scale_)\n","\n","X_scaled = scaler.transform(X_train)\n","print('X_scaled=')\n","print(X_scaled)"],"metadata":{"id":"AXOU9PwRlyn0","executionInfo":{"status":"ok","timestamp":1731928337728,"user_tz":-60,"elapsed":6412,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}},"outputId":"f0402f51-b505-42a8-e9ed-fc18dde83a3d","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["scaler.mean= [1.         0.         0.33333333]\n","scaler.scale= [0.81649658 0.81649658 1.24721913]\n","X_scaled=\n","[[ 0.         -1.22474487  1.33630621]\n"," [ 1.22474487  0.         -0.26726124]\n"," [-1.22474487  1.22474487 -1.06904497]]\n"]}]},{"cell_type":"markdown","source":["Scaled data has zero mean and unit variance:\n"],"metadata":{"id":"SfQC62yvmdlD"}},{"cell_type":"code","source":["X_scaled.mean(axis=0)"],"metadata":{"id":"dd5xPmmCmgE5","executionInfo":{"status":"ok","timestamp":1731928337729,"user_tz":-60,"elapsed":14,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}},"outputId":"62f2513b-79a4-44d4-fb8b-416966f57a23","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0.])"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["X_scaled.std(axis=0)\n"],"metadata":{"id":"rJrE5U1xmhLR","executionInfo":{"status":"ok","timestamp":1731928337729,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}},"outputId":"facc2b7a-cba0-438a-9aee-268132517920","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 1., 1.])"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["## 2. Scaling features to a range\n","An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using MinMaxScaler or MaxAbsScaler, respectively.\n","\n","The motivation to use this scaling includes robustness to very small standard deviations of features and preserving zero entries in sparse data.\n","\n","The `MinMaxScaler` estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n","\n","The transformation is given by\n","\n","`X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))`\n","\n","`X_scaled = X_std * (max - min) + min`\n","\n","where min, max = feature_range.\n","\n","Here is an example to scale a toy data matrix to the [0, 1] range."],"metadata":{"id":"bsYBu2UHm0uz"}},{"cell_type":"code","source":["X_train = np.array([[ 1., -1.,  2.],\n","                    [ 2.,  0.,  0.],\n","                    [ 0.,  1., -1.]])\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","X_train_minmax = min_max_scaler.fit_transform(X_train)\n","print(X_train_minmax)"],"metadata":{"id":"CGPgF8JznFX6","executionInfo":{"status":"ok","timestamp":1731928337729,"user_tz":-60,"elapsed":8,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}},"outputId":"c6844cdb-030e-4eab-8d8c-543f210d7975","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.5        0.         1.        ]\n"," [1.         0.5        0.33333333]\n"," [0.         1.         0.        ]]\n"]}]},{"cell_type":"markdown","source":["The other estimator, `MaxAbsScaler` works in a very similar fashion, but scales in a way that the training data lies within the range [-1, 1] by dividing through the largest maximum value in each feature. It is meant for data that is already centered at zero or sparse data.\n","\n","Here is how to use the toy data from the previous example with this scaler:"],"metadata":{"id":"a9Yk9g2coU7V"}},{"cell_type":"code","source":["X_train = np.array([[ 1., -1.,  2.],\n","                    [ 2.,  0.,  0.],\n","                    [ 0.,  1., -1.]])\n","\n","max_abs_scaler = preprocessing.MaxAbsScaler()\n","X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n","print(X_train_maxabs)\n","\n","X_test = np.array([[ -3., -1.,  4.]])\n","X_test_maxabs = max_abs_scaler.transform(X_test)\n","print(X_test_maxabs)\n","print(max_abs_scaler.scale_)\n"],"metadata":{"id":"d8irEFCfoqAx","executionInfo":{"status":"ok","timestamp":1731928337729,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ricard Abril Ferreres","userId":"11904291188846620981"}},"outputId":"e386c2f1-5d16-40fe-eae9-d6676ce4357f","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.5 -1.   1. ]\n"," [ 1.   0.   0. ]\n"," [ 0.   1.  -0.5]]\n","[[-1.5 -1.   2. ]]\n","[2. 1. 2.]\n"]}]},{"cell_type":"markdown","source":["## 3. Scaling data with outliers\n","\n","If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use `RobustScaler` as a drop-in replacement instead. It uses more robust estimates for the center and range of your data.\n","\n","This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n","\n","Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method.\n","\n","Check [this example](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#plot-all-scaling-robust-scaler-section) to compare the effect of different scalers on data with outliers."],"metadata":{"id":"SBxbwcjJo_8u"}}]}